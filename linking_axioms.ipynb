{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:17:33.193532Z","iopub.execute_input":"2025-10-16T06:17:33.193783Z","iopub.status.idle":"2025-10-16T06:17:34.690579Z","shell.execute_reply.started":"2025-10-16T06:17:33.193759Z","shell.execute_reply":"2025-10-16T06:17:34.689794Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# LogicLlama NL to FOL ","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes peft evaluate \n!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:17:34.691419Z","iopub.execute_input":"2025-10-16T06:17:34.691818Z","iopub.status.idle":"2025-10-16T06:18:53.830286Z","shell.execute_reply.started":"2025-10-16T06:17:34.691793Z","shell.execute_reply":"2025-10-16T06:18:53.829541Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.1)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Core libraries\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # or CPU version if no GPU\n\n# Transformers and PEFT\n# !pip install transformers==4.34.0\n# !pip install peft\n\n# Utilities\n!pip install tqdm\n!pip install numpy\n\n# Optional (for data handling)\n!pip install datasets\n!pip install sentencepiece  # needed for some LLaMA tokenizers\n\n# If you’re running the eval scripts or working with JSON datasets\n!pip install jsonlines\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:18:53.832458Z","iopub.execute_input":"2025-10-16T06:18:53.832705Z","iopub.status.idle":"2025-10-16T06:19:12.723474Z","shell.execute_reply.started":"2025-10-16T06:18:53.832683Z","shell.execute_reply":"2025-10-16T06:19:12.722817Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nCollecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines) (25.3.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git clone https://github.com/gblackout/LogicLLaMA.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:19:12.724419Z","iopub.execute_input":"2025-10-16T06:19:12.724749Z","iopub.status.idle":"2025-10-16T06:19:13.442690Z","shell.execute_reply.started":"2025-10-16T06:19:12.724715Z","shell.execute_reply":"2025-10-16T06:19:13.441972Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'LogicLLaMA'...\nremote: Enumerating objects: 33, done.\u001b[K\nremote: Counting objects: 100% (33/33), done.\u001b[K\nremote: Compressing objects: 100% (28/28), done.\u001b[K\nremote: Total 33 (delta 6), reused 29 (delta 4), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (33/33), 37.06 KiB | 2.06 MiB/s, done.\nResolving deltas: 100% (6/6), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/LogicLLaMA')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:19:13.443562Z","iopub.execute_input":"2025-10-16T06:19:13.443777Z","iopub.status.idle":"2025-10-16T06:19:13.447750Z","shell.execute_reply.started":"2025-10-16T06:19:13.443755Z","shell.execute_reply":"2025-10-16T06:19:13.446966Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install -r /kaggle/working/LogicLLaMA/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:19:13.448420Z","iopub.execute_input":"2025-10-16T06:19:13.448614Z","iopub.status.idle":"2025-10-16T06:19:51.937317Z","shell.execute_reply.started":"2025-10-16T06:19:13.448564Z","shell.execute_reply":"2025-10-16T06:19:51.936387Z"}},"outputs":[{"name":"stdout","text":"Collecting peft@ git+https://github.com/huggingface/peft.git@b1059b73aab9043b118ff19b0cf96263ea86248a (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 87))\n  Cloning https://github.com/huggingface/peft.git (to revision b1059b73aab9043b118ff19b0cf96263ea86248a) to /tmp/pip-install-kna74lvv/peft_82839325b3f641518e1bd6038b682668\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-install-kna74lvv/peft_82839325b3f641518e1bd6038b682668\n  Running command git rev-parse -q --verify 'sha^b1059b73aab9043b118ff19b0cf96263ea86248a'\n  Running command git fetch -q https://github.com/huggingface/peft.git b1059b73aab9043b118ff19b0cf96263ea86248a\n  Running command git checkout -q b1059b73aab9043b118ff19b0cf96263ea86248a\n  Resolved https://github.com/huggingface/peft.git to commit b1059b73aab9043b118ff19b0cf96263ea86248a\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting transformers@ git+https://github.com/huggingface/transformers.git@3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 135))\n  Cloning https://github.com/huggingface/transformers.git (to revision 3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec) to /tmp/pip-install-kna74lvv/transformers_362ac94a065044fbb0b3ede353bf65fd\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-kna74lvv/transformers_362ac94a065044fbb0b3ede353bf65fd\n  Running command git rev-parse -q --verify 'sha^3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec'\n  Running command git fetch -q https://github.com/huggingface/transformers.git 3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec\n  Running command git checkout -q 3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec\n  Resolved https://github.com/huggingface/transformers.git to commit 3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting trl@ git+https://github.com/lvwerra/trl.git@a004b02c4a058051cd6fa56073ff12b2d153ffc6 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 136))\n  Cloning https://github.com/lvwerra/trl.git (to revision a004b02c4a058051cd6fa56073ff12b2d153ffc6) to /tmp/pip-install-kna74lvv/trl_481f6a45ab074d82ab9daec1118f4612\n  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-install-kna74lvv/trl_481f6a45ab074d82ab9daec1118f4612\n  Running command git rev-parse -q --verify 'sha^a004b02c4a058051cd6fa56073ff12b2d153ffc6'\n  Running command git fetch -q https://github.com/lvwerra/trl.git a004b02c4a058051cd6fa56073ff12b2d153ffc6\n  Running command git checkout -q a004b02c4a058051cd6fa56073ff12b2d153ffc6\n  Resolved https://github.com/lvwerra/trl.git to commit a004b02c4a058051cd6fa56073ff12b2d153ffc6\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting accelerate==0.16.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 1))\n  Downloading accelerate-0.16.0-py3-none-any.whl.metadata (15 kB)\nCollecting aiohttp==3.8.4 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 2))\n  Downloading aiohttp-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nCollecting aiosignal==1.3.1 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 3))\n  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\nCollecting anyio==3.6.2 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 4))\n  Downloading anyio-3.6.2-py3-none-any.whl.metadata (4.7 kB)\nCollecting appdirs==1.4.4 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 5))\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting argon2-cffi==21.3.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 6))\n  Downloading argon2_cffi-21.3.0-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: argon2-cffi-bindings==21.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 7)) (21.2.0)\nCollecting async-timeout==4.0.2 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 8))\n  Downloading async_timeout-4.0.2-py3-none-any.whl.metadata (4.2 kB)\nCollecting asynctest==0.13.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 9))\n  Downloading asynctest-0.13.0-py3-none-any.whl.metadata (5.4 kB)\nCollecting attrs==22.2.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 10))\n  Downloading attrs-22.2.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 11)) (0.2.0)\nCollecting beartype==0.12.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 12))\n  Downloading beartype-0.12.0-py3-none-any.whl.metadata (324 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting beautifulsoup4==4.11.2 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 13))\n  Downloading beautifulsoup4-4.11.2-py3-none-any.whl.metadata (3.5 kB)\nCollecting bitsandbytes==0.40.1.post1 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 14))\n  Downloading bitsandbytes-0.40.1.post1-py3-none-any.whl.metadata (9.8 kB)\nCollecting bleach==6.0.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 15))\n  Downloading bleach-6.0.0-py3-none-any.whl.metadata (29 kB)\nCollecting boto==2.49.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 16))\n  Downloading boto-2.49.0-py2.py3-none-any.whl.metadata (7.3 kB)\nCollecting cffi==1.15.1 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 17))\n  Downloading cffi-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting charset-normalizer==3.0.1 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 18))\n  Downloading charset_normalizer-3.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nCollecting click==8.1.3 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 19))\n  Downloading click-8.1.3-py3-none-any.whl.metadata (3.2 kB)\nCollecting cycler==0.11.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 20))\n  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\nCollecting datasets==2.10.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 21))\n  Downloading datasets-2.10.0-py3-none-any.whl.metadata (20 kB)\nCollecting debugpy==1.6.6 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 22))\n  Downloading debugpy-1.6.6-py2.py3-none-any.whl.metadata (1.1 kB)\nCollecting decorator==5.1.1 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 23))\n  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 24)) (0.7.1)\nCollecting dill==0.3.6 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 25))\n  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\nCollecting docker-pycreds==0.4.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 26))\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting einops==0.6.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 27))\n  Downloading einops-0.6.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: entrypoints==0.4 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 28)) (0.4)\nCollecting et-xmlfile==1.1.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 29))\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting evaluate==0.4.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 30))\n  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\nCollecting fastjsonschema==2.16.3 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 31))\n  Downloading fastjsonschema-2.16.3-py3-none-any.whl.metadata (2.0 kB)\nCollecting filelock==3.9.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 32))\n  Downloading filelock-3.9.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting fire==0.5.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 33))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fonttools==4.38.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 34))\n  Downloading fonttools-4.38.0-py3-none-any.whl.metadata (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting frozenlist==1.3.3 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 35))\n  Downloading frozenlist-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nCollecting fsspec==2023.1.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 36))\n  Downloading fsspec-2023.1.0-py3-none-any.whl.metadata (5.5 kB)\nCollecting gitdb==4.0.10 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 37))\n  Downloading gitdb-4.0.10-py3-none-any.whl.metadata (1.1 kB)\nCollecting GitPython==3.1.31 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 38))\n  Downloading GitPython-3.1.31-py3-none-any.whl.metadata (1.3 kB)\nCollecting huggingface-hub==0.13.3 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 39))\n  Downloading huggingface_hub-0.13.3-py3-none-any.whl.metadata (7.5 kB)\nCollecting idna==3.4 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 40))\n  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\nCollecting importlib-metadata==6.0.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 41))\n  Downloading importlib_metadata-6.0.0-py3-none-any.whl.metadata (5.0 kB)\nCollecting importlib-resources==5.12.0 (from -r /kaggle/working/LogicLLaMA/requirements.txt (line 42))\n  Downloading importlib_resources-5.12.0-py3-none-any.whl.metadata (4.1 kB)\n\u001b[31mERROR: Could not find a version that satisfies the requirement install==1.3.5 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for install==1.3.5\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install python-Levenshtein","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:19:51.938397Z","iopub.execute_input":"2025-10-16T06:19:51.938645Z","iopub.status.idle":"2025-10-16T06:19:57.278188Z","shell.execute_reply.started":"2025-10-16T06:19:51.938610Z","shell.execute_reply":"2025-10-16T06:19:57.277152Z"}},"outputs":[{"name":"stdout","text":"Collecting python-Levenshtein\n  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.27.1 (from python-Levenshtein)\n  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n  Downloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.14.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom functools import partial\nfrom transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\nfrom peft import PeftModel, prepare_model_for_kbit_training \nfrom utils import TranslationDataPreparer, ContinuousCorrectionDataPreparer, make_parent_dirs\nfrom fol_parser import parse_text_FOL_to_tree\nfrom generate import llama_generate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:19:57.279293Z","iopub.execute_input":"2025-10-16T06:19:57.279497Z","iopub.status.idle":"2025-10-16T06:20:23.613039Z","shell.execute_reply.started":"2025-10-16T06:19:57.279473Z","shell.execute_reply":"2025-10-16T06:20:23.612370Z"}},"outputs":[{"name":"stderr","text":"2025-10-16 06:20:10.834155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760595611.049363      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760595611.110594      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:20:23.615744Z","iopub.execute_input":"2025-10-16T06:20:23.616270Z","iopub.status.idle":"2025-10-16T06:20:26.939749Z","shell.execute_reply.started":"2025-10-16T06:20:23.616249Z","shell.execute_reply":"2025-10-16T06:20:26.938980Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:20:26.940797Z","iopub.execute_input":"2025-10-16T06:20:26.941018Z","iopub.status.idle":"2025-10-16T06:20:27.078912Z","shell.execute_reply.started":"2025-10-16T06:20:26.940995Z","shell.execute_reply":"2025-10-16T06:20:27.078307Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\nimport torch\n\n# Config\nbase_model = \"meta-llama/Llama-2-7b-hf\"\nmax_output_len = 128\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_auth_token=True)\ntokenizer.add_special_tokens({\"eos_token\": \"</s>\", \"bos_token\": \"<s>\", \"unk_token\": \"<unk>\", \"pad_token\": \"<unk>\"})\ntokenizer.padding_side = \"left\"\n\n# Generation config\ngeneration_config = GenerationConfig(\n    temperature=0.1, top_p=0.75, top_k=40, num_beams=1, max_new_tokens=max_output_len\n)\n\n# BitsAndBytes config\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\n\n# Load model\nllama_model = LlamaForCausalLM.from_pretrained(\n    base_model,\n    #device_map=\"auto\",\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    use_auth_token=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:20:27.079684Z","iopub.execute_input":"2025-10-16T06:20:27.079924Z","iopub.status.idle":"2025-10-16T06:22:56.082551Z","shell.execute_reply.started":"2025-10-16T06:20:27.079902Z","shell.execute_reply":"2025-10-16T06:22:56.082002Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d30eba5368d946ba8db039e52b6c3dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203403fc42994149b73424760fbc2814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6531c1f5229406eb4ee356ad6024d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a57e8c84b7490ba1d74e239e62f069"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4191: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8ee9ad4d33841df8d7e1f06d75838be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7fed31585ea4d998777a1101c408c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ab0e5227df4552bb868154e93f0aa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78730cab8c9547c98ec38952da372219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b77b7ebca840b796dcccf16ef40060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77d0551286774eaf839c09b8d7b53951"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602e42cd779f46f797413f1816c020d0"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nllama_model = prepare_model_for_kbit_training(llama_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:56.083293Z","iopub.execute_input":"2025-10-16T06:22:56.083473Z","iopub.status.idle":"2025-10-16T06:22:56.107312Z","shell.execute_reply.started":"2025-10-16T06:22:56.083457Z","shell.execute_reply":"2025-10-16T06:22:56.106823Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"llama_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:56.108030Z","iopub.execute_input":"2025-10-16T06:22:56.108282Z","iopub.status.idle":"2025-10-16T06:22:56.114573Z","shell.execute_reply.started":"2025-10-16T06:22:56.108261Z","shell.execute_reply":"2025-10-16T06:22:56.113942Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"peft_path='yuan-yang/LogicLLaMA-7b-direct-translate-delta-v0.1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:56.115474Z","iopub.execute_input":"2025-10-16T06:22:56.115972Z","iopub.status.idle":"2025-10-16T06:22:56.124572Z","shell.execute_reply.started":"2025-10-16T06:22:56.115946Z","shell.execute_reply":"2025-10-16T06:22:56.123960Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PeftModel.from_pretrained(\n    llama_model,\n    peft_path,\n    torch_dtype=torch.float16\n)\nmodel.to(\"cuda:0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:56.125506Z","iopub.execute_input":"2025-10-16T06:22:56.125857Z","iopub.status.idle":"2025-10-16T06:22:59.259273Z","shell.execute_reply.started":"2025-10-16T06:22:56.125839Z","shell.execute_reply":"2025-10-16T06:22:59.258700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d8fa3b16cf4b319fb502636c650c8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/80.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54db09e664974f3c86e65319007e708b"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:59.260030Z","iopub.execute_input":"2025-10-16T06:22:59.260283Z","iopub.status.idle":"2025-10-16T06:22:59.268453Z","shell.execute_reply.started":"2025-10-16T06:22:59.260247Z","shell.execute_reply":"2025-10-16T06:22:59.267767Z"}},"outputs":[{"name":"stdout","text":"trainable params: 0 || all params: 6,758,404,096 || trainable%: 0.0000\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"prompt_template_path= \"/kaggle/working/LogicLLaMA/data/prompt_templates\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:59.269296Z","iopub.execute_input":"2025-10-16T06:22:59.269574Z","iopub.status.idle":"2025-10-16T06:22:59.284904Z","shell.execute_reply.started":"2025-10-16T06:22:59.269553Z","shell.execute_reply":"2025-10-16T06:22:59.284361Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"prompt_template_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:59.285911Z","iopub.execute_input":"2025-10-16T06:22:59.286169Z","iopub.status.idle":"2025-10-16T06:22:59.296900Z","shell.execute_reply.started":"2025-10-16T06:22:59.286146Z","shell.execute_reply":"2025-10-16T06:22:59.296118Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/LogicLLaMA/data/prompt_templates'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"data_preparer = TranslationDataPreparer(\n    prompt_template_path,\n    tokenizer,\n    False,\n    256 \n)\n\nprepare_input = partial(\n    data_preparer.prepare_input,\n    **{\"nl_key\": \"NL\"},\n    add_eos_token=False,\n    eval_mode=True,\n    return_tensors='pt'\n)\n\nsimple_generate = partial(\n    llama_generate,\n    llama_model=model,\n    data_preparer=data_preparer,\n    max_new_tokens=max_output_len,\n    generation_config=generation_config,\n    prepare_input=prepare_input,\n    return_tensors=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:59.297670Z","iopub.execute_input":"2025-10-16T06:22:59.297993Z","iopub.status.idle":"2025-10-16T06:22:59.311045Z","shell.execute_reply.started":"2025-10-16T06:22:59.297969Z","shell.execute_reply":"2025-10-16T06:22:59.310387Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"data_point = {'NL': 'The system must make investment recommendations every hour but must not make any recommendations at any hourly interval.'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:59.311681Z","iopub.execute_input":"2025-10-16T06:22:59.311904Z","iopub.status.idle":"2025-10-16T06:22:59.317443Z","shell.execute_reply.started":"2025-10-16T06:22:59.311889Z","shell.execute_reply":"2025-10-16T06:22:59.316924Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"full_resp_str, resp_parts = simple_generate(input_str=data_point)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:22:59.318230Z","iopub.execute_input":"2025-10-16T06:22:59.318481Z","iopub.status.idle":"2025-10-16T06:23:15.782974Z","shell.execute_reply.started":"2025-10-16T06:22:59.318460Z","shell.execute_reply":"2025-10-16T06:23:15.782378Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"resp_parts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:15.783656Z","iopub.execute_input":"2025-10-16T06:23:15.783827Z","iopub.status.idle":"2025-10-16T06:23:15.788464Z","shell.execute_reply.started":"2025-10-16T06:23:15.783813Z","shell.execute_reply":"2025-10-16T06:23:15.787928Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['N/A\\n\\n',\n '∀x (System(x) ∧ InvestmentRecommendations(x) → (MakesRecommendationsEveryHour(x) ∧ ¬MakesRecommendationsAtAnyHourlyInterval(x)))']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"rule_str = resp_parts[1]\nrule_str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:15.789189Z","iopub.execute_input":"2025-10-16T06:23:15.789428Z","iopub.status.idle":"2025-10-16T06:23:15.800360Z","shell.execute_reply.started":"2025-10-16T06:23:15.789406Z","shell.execute_reply":"2025-10-16T06:23:15.799852Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'∀x (System(x) ∧ InvestmentRecommendations(x) → (MakesRecommendationsEveryHour(x) ∧ ¬MakesRecommendationsAtAnyHourlyInterval(x)))'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"data_point = {'NL': 'if the user logs in, there must have user profile'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:15.801028Z","iopub.execute_input":"2025-10-16T06:23:15.801231Z","iopub.status.idle":"2025-10-16T06:23:15.812382Z","shell.execute_reply.started":"2025-10-16T06:23:15.801216Z","shell.execute_reply":"2025-10-16T06:23:15.811825Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"examples = [\n    {\"NL\": \"System must guarantee an annualized return of exactly 10 percent for all customers regardless of their risk profile.\"},\n    {\"NL\": \"For all customers, the system must guarantee a portfolio return of exactly 10 percent per annum.\"},\n    {\"NL\": \"For all high-risk customers, the system must ensure the portfolio return is strictly less than 10 percent per annum to comply with internal risk policies.\"},\n   \n]\n\nresults = []\nfor dp in examples:\n    try:\n        full_resp_str, resp_parts = simple_generate(input_str=dp)\n        if resp_parts and len(resp_parts) > 1:\n            print(f\"NL: {dp['NL']}\")\n            print(f\"FOL: {resp_parts[1]}\\n\")\n            results.append(resp_parts[1])\n        else:\n            print(f\"NL: {dp['NL']}\")\n            print(\"FOL: [None returned]\\n\")\n    except Exception as e:\n        print(f\"Error for input {dp['NL']}: {e}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:15.813060Z","iopub.execute_input":"2025-10-16T06:23:15.813298Z","iopub.status.idle":"2025-10-16T06:23:46.472384Z","shell.execute_reply.started":"2025-10-16T06:23:15.813269Z","shell.execute_reply":"2025-10-16T06:23:46.471753Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"NL: System must guarantee an annualized return of exactly 10 percent for all customers regardless of their risk profile.\nFOL: ∀x (System(x) ∧ Customer(y) ∧ RiskProfile(z) → GuaranteesAnnualizedReturn(x, y, z, 10))\n\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"NL: For all customers, the system must guarantee a portfolio return of exactly 10 percent per annum.\nFOL: ∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10))\n\nNL: For all high-risk customers, the system must ensure the portfolio return is strictly less than 10 percent per annum to comply with internal risk policies.\nFOL: ∀x (HighRiskCustomer(x) → (PortfolioReturnLessThan10PercentPerAnnum(x)))\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# LogicLlama output to SMT Feedbable Translator tool ","metadata":{}},{"cell_type":"code","source":"∀x∀y∀z (\n    (System(x) ∧ Customer(y) ∧ RiskProfile(z))\n        → GuaranteesAnnualizedReturn(x, y, z, 10)\n)\n∧\n∀x (\n    Customer(x) → GuaranteedPortfolioReturn(x, 10)\n)\n∧\n∀x (\n    HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x)\n)\n∧\n∀x (\n    HighRiskCustomer(x) → Customer(x)\n)\n∧\n∀x (\n    (GuaranteedPortfolioReturn(x, 10) → PortfolioReturn(x) = 10)\n)\n∧\n∀x (\n    (PortfolioReturnLessThan10PercentPerAnnum(x) → PortfolioReturn(x) < 10)\n)\n∧\n∃x (\n    HighRiskCustomer(x)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.473144Z","iopub.execute_input":"2025-10-16T06:23:46.473357Z","iopub.status.idle":"2025-10-16T06:23:46.482531Z","shell.execute_reply.started":"2025-10-16T06:23:46.473342Z","shell.execute_reply":"2025-10-16T06:23:46.480375Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/3985677177.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ∀x∀y∀z (\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '∀' (U+2200)\n"],"ename":"SyntaxError","evalue":"invalid character '∀' (U+2200) (3985677177.py, line 1)","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"!pip install google-generativeai sympy python-sat\n!pip install z3-solver google-generativeai python-dotenv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:24:07.481407Z","iopub.execute_input":"2025-10-16T06:24:07.481743Z","iopub.status.idle":"2025-10-16T06:24:16.637981Z","shell.execute_reply.started":"2025-10-16T06:24:07.481722Z","shell.execute_reply":"2025-10-16T06:24:16.637193Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\nCollecting python-sat\n  Downloading python_sat-1.8.dev24-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.173.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.40.3)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from python-sat) (1.17.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\nDownloading python_sat-1.8.dev24-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-sat\nSuccessfully installed python-sat-1.8.dev24\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting z3-solver\n  Downloading z3_solver-4.15.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (602 bytes)\nRequirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\nCollecting python-dotenv\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.173.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.40.3)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\nDownloading z3_solver-4.15.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: z3-solver, python-dotenv\nSuccessfully installed python-dotenv-1.1.1 z3-solver-4.15.3.0\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\n\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/kaggle/input/my-service-account.json\"\ngenai.configure() \nGOOGLE_API_KEY=\"AIzaSyCCJvm8lwqFKS-c0sszXXknyQgVK1GSRHU\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:24:16.639548Z","iopub.execute_input":"2025-10-16T06:24:16.639791Z","iopub.status.idle":"2025-10-16T06:24:18.107766Z","shell.execute_reply.started":"2025-10-16T06:24:16.639770Z","shell.execute_reply":"2025-10-16T06:24:18.107002Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"genai.configure(api_key=GOOGLE_API_KEY)\nmodel = genai.GenerativeModel(\"gemini-2.5-flash\")  \nresponse = model.generate_content(\"Hello from Kaggle\")\nresponse.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:24:18.108607Z","iopub.execute_input":"2025-10-16T06:24:18.108821Z","iopub.status.idle":"2025-10-16T06:24:22.220163Z","shell.execute_reply.started":"2025-10-16T06:24:18.108805Z","shell.execute_reply":"2025-10-16T06:24:22.219570Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'Hello there! Great to hear from a Kaggler.\\n\\nAre you working on a competition, exploring a dataset, brainstorming a project, or have a specific data science or machine learning question?\\n\\nLet me know how I can help!'"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import os\nimport re\nimport logging\nfrom google.generativeai import types\nfrom typing import Tuple, Optional, Dict, Any, List\nfrom dataclasses import dataclass\nimport google.generativeai as genai\nfrom z3 import *\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SATResult:\n    \"\"\"Enhanced result container for SAT checking\"\"\"\n    status: str  # SAT, UNSAT, UNKNOWN, ERROR\n    formula: str\n    z3_code: str\n    model: Optional[Any] = None\n    counterexample: Optional[Dict[str, Any]] = None\n    error_message: Optional[str] = None\n    processing_time: Optional[float] = None\n\nclass GeminiFlashTranslator:\n    \"\"\"Handles translation of FOL to Z3 using Gemini Flash 2.5\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        genai.configure(api_key=api_key)\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\n        \n        # Enhanced prompt for better translation with assert_and_track\n        self.translation_prompt = \"\"\"\nCRITICAL: Convert this First-Order Logic formula to executable Z3 Python code. \nOUTPUT ONLY RAW PYTHON CODE - NO EXPLANATIONS, NO MARKDOWN.\n\nFORMULA: {fol_formula}\n\nREQUIREMENTS:\n1. Import: from z3 import *\n2. Create solver with unsat_core enabled: s = Solver()\n   s.set(unsat_core=True)\n3. Define all variables as Int or Bool\n4. Define predicates as Function(name, IntSort(), BoolSort())\n5. Add formulas using assert_and_track with unique labels:\n   s.assert_and_track(formula1, \"rule1\")\n   s.assert_and_track(formula2, \"rule2\")\n6. Check satisfiability: result = s.check()\n7. For quantifiers: Use ForAll([x], ...) and Exists([x], ...)\n8. Map operators: → to Implies, ∧ to And, ∨ to Or, ¬ to Not, ⊕ to Xor, ↔ to ==\n\nIMPORTANT: Use this exact structure:\nfrom z3 import *\ns = Solver()\ns.set(unsat_core=True)\n\n# Define variables and predicates\nx = Int('x')\nCreatedRepo = Function('CreatedRepo', IntSort(), BoolSort())\nHuman = Function('Human', IntSort(), BoolSort())\nAlien = Function('Alien', IntSort(), BoolSort())\n\n# Add the formula with tracking\ns.assert_and_track(ForAll([x], Implies(CreatedRepo(x), Xor(Human(x), Alien(x)))), \"rule1\")\n\n# Check satisfiability\nresult = s.check()\nif result == sat:\n    model = s.model()\nelif result == unsat:\n    print(\"UNSAT\")\nelse:\n    print(\"UNKNOWN\")\n\nCODE:\n\"\"\"\n    \n    def translate_to_z3(self, fol_formula: str) -> Tuple[str, Optional[str]]:\n        \"\"\"Translate FOL to Z3 code using Gemini Flash 2.5\"\"\"\n        try:\n            prompt = self.translation_prompt.format(fol_formula=fol_formula)\n\n            generation_config = genai.types.GenerationConfig(\n            max_output_tokens=20000, temperature=0.1, top_p=0.8, top_k=40,\n            candidate_count=1\n            )\n            \n            response = self.model.generate_content(\n                prompt, \n                generation_config=generation_config\n            )\n            \n            if not response.text:\n                raise ValueError(\"Empty response from Gemini API\")\n                \n            z3_code = self._clean_gemini_response(response.text)\n            \n            # Ensure unsat_core tracking is enabled\n            z3_code = self._ensure_unsat_tracking(z3_code)\n            \n            if self._validate_z3_code(z3_code):\n                logger.info(\"Successfully generated Z3 code\")\n                return z3_code, None\n            else:\n                logger.warning(\"Generated code failed validation, attempting repair\")\n                repaired_code = self._repair_z3_code(z3_code, fol_formula)\n                if self._validate_z3_code(repaired_code):\n                    return repaired_code, None\n                return None, \"Generated Z3 code failed validation and repair\"\n                \n        except Exception as e:\n            logger.error(f\"Gemini translation failed: {str(e)}\")\n            return None, f\"Translation error: {str(e)}\"\n    \n    def _clean_gemini_response(self, response: str) -> str:\n        \"\"\"Clean and extract Z3 code from Gemini response\"\"\"\n        cleaned = re.sub(r'```python\\n?(.*?)\\n?```', r'\\1', response, flags=re.DOTALL)\n        cleaned = re.sub(r'```\\n?(.*?)\\n?```', r'\\1', cleaned, flags=re.DOTALL)\n        cleaned = cleaned.strip().strip('\"').strip(\"'\")\n        \n        if \"from z3 import *\" not in cleaned:\n            cleaned = \"from z3 import *\\n\" + cleaned\n            \n        return cleaned\n    \n    def _ensure_unsat_tracking(self, code: str) -> str:\n        \"\"\"Ensure unsat_core tracking is enabled in the solver\"\"\"\n        if \"s.set(unsat_core=True)\" not in code:\n            # Add unsat_core setting right after Solver creation\n            if \"s = Solver()\" in code:\n                code = code.replace(\"s = Solver()\", \"s = Solver()\\ns.set(unsat_core=True)\")\n            else:\n                # Add both if solver creation is missing\n                code = \"s = Solver()\\ns.set(unsat_core=True)\\n\" + code\n        return code\n    \n    def _repair_z3_code(self, code: str, original_formula: str) -> str:\n        \"\"\"Attempt to repair common issues in generated Z3 code\"\"\"\n        repaired = code\n        \n        # Extract variable names from formula\n        variables = set(re.findall(r'\\b([a-z])\\b', original_formula))\n        predicates = set(re.findall(r'([A-Z][a-zA-Z]*)\\(', original_formula))\n        \n        # Add missing variable definitions\n        for var in variables:\n            if f\"{var} = Int('{var}')\" not in repaired and f\"{var} = Int(\" not in repaired:\n                repaired = f\"{var} = Int('{var}')\\n\" + repaired\n        \n        # Add missing predicate definitions\n        for pred in predicates:\n            if f\"{pred} = Function('{pred}'\" not in repaired:\n                repaired = f\"{pred} = Function('{pred}', IntSort(), BoolSort())\\n\" + repaired\n        \n        # Convert s.add() to s.assert_and_track()\n        rule_counter = 1\n        def add_to_track(match):\n            nonlocal rule_counter\n            text = f's.assert_and_track({match.group(1)}, \"rule{rule_counter}\")'\n            rule_counter += 1\n            return text\n        \n        if \"s.add(\" in repaired:\n            repaired = re.sub(r's\\.add\\((.*?)\\)', add_to_track, repaired)\n        \n        # Ensure unsat tracking is enabled\n        repaired = self._ensure_unsat_tracking(repaired)\n        \n        # Ensure solver check is present\n        if \"s.check()\" not in repaired:\n            repaired += \"\\nresult = s.check()\\n\"\n            \n        return repaired\n    \n    def _validate_z3_code(self, code: str) -> bool:\n        \"\"\"Basic validation of generated Z3 code\"\"\"\n        required_elements = [\n            \"from z3 import\",\n            \"Solver()\",\n        ]\n        return all(elem in code for elem in required_elements)\n\n\nclass EnterpriseFOLChecker:\n    \"\"\"Enterprise-grade FOL satisfiability checker with Gemini integration\"\"\"\n    \n    def __init__(self, gemini_api_key: str):\n        self.translator = GeminiFlashTranslator(gemini_api_key)\n        \n    def check_satisfiability(self, fol_formula: str) -> SATResult:\n        \"\"\"\n        Main method to check FOL satisfiability with comprehensive error handling\n        \"\"\"\n        import time\n        start_time = time.time()\n        \n        try:\n            # Step 1: Translate FOL to Z3 using Gemini\n            logger.info(f\"Translating FOL formula: {fol_formula}\")\n            z3_code, error = self.translator.translate_to_z3(fol_formula)\n            \n            if error:\n                return SATResult(\n                    status=\"ERROR\",\n                    formula=fol_formula,\n                    z3_code=\"\",\n                    error_message=f\"Translation failed: {error}\",\n                    processing_time=time.time() - start_time\n                )\n            \n            # Step 2: Execute Z3 code in a controlled environment\n            logger.info(\"Executing Z3 code\")\n            result = self._execute_z3_code(z3_code, fol_formula)\n            result.processing_time = time.time() - start_time\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Unexpected error during SAT checking: {str(e)}\")\n            return SATResult(\n                status=\"ERROR\",\n                formula=fol_formula,\n                z3_code=\"\",\n                error_message=f\"Runtime error: {str(e)}\",\n                processing_time=time.time() - start_time\n            )\n    \n    def _execute_z3_code(self, z3_code: str, original_formula: str) -> SATResult:\n        \"\"\"Execute Z3 code in a proper execution environment\"\"\"\n        try:\n            # Create a proper execution environment with all necessary builtins\n            local_vars = {}\n            \n            # Import all necessary Z3 components directly\n            from z3 import (Solver, Int, Bool, Function, ForAll, Exists, Implies, \n                          And, Or, Not, Xor, sat, unsat, unknown, IntSort, BoolSort)\n            \n            # Create a safe environment with necessary builtins\n            safe_builtins = {\n                'print': print,\n                'str': str,\n                'int': int,\n                'bool': bool,\n                'list': list,\n                'dict': dict,\n                'range': range,\n                'len': len,\n            }\n            \n            # Add Z3 components to local vars\n            local_vars.update({\n                'Solver': Solver,\n                'Int': Int,\n                'Bool': Bool,\n                'Function': Function,\n                'ForAll': ForAll,\n                'Exists': Exists,\n                'Implies': Implies,\n                'And': And,\n                'Or': Or,\n                'Not': Not,\n                'Xor': Xor,\n                'sat': sat,\n                'unsat': unsat,\n                'unknown': unknown,\n                'IntSort': IntSort,\n                'BoolSort': BoolSort,\n            })\n            \n            local_vars.update(safe_builtins)\n            \n            # Execute the generated Z3 code\n            exec(z3_code, local_vars, local_vars)\n            \n            # Extract results\n            s = local_vars.get('s')\n            result = local_vars.get('result')\n            model = local_vars.get('model')\n            \n            if not s:\n                return SATResult(\n                    status=\"ERROR\",\n                    formula=original_formula,\n                    z3_code=z3_code,\n                    error_message=\"Solver not found in generated code\"\n                )\n            \n            # If result wasn't captured, check directly\n            if result is None:\n                result = s.check()\n            \n            if result == sat:\n                if model is None:\n                    model = s.model()\n                return SATResult(\n                    status=\"SAT\",\n                    formula=original_formula,\n                    z3_code=z3_code,\n                    model=model\n                )\n                \n            elif result == unsat:\n                # Extract unsat core from the solver\n                counterexample = self._extract_unsat_info(s)\n                return SATResult(\n                    status=\"UNSAT\",\n                    formula=original_formula,\n                    z3_code=z3_code,\n                    counterexample=counterexample\n                )\n                \n            else:\n                return SATResult(\n                    status=\"UNKNOWN\",\n                    formula=original_formula,\n                    z3_code=z3_code\n                )\n                \n        except Exception as e:\n            logger.error(f\"Z3 execution error: {str(e)}\")\n            return SATResult(\n                status=\"ERROR\",\n                formula=original_formula,\n                z3_code=z3_code,\n                error_message=f\"Z3 execution error: {str(e)}\"\n            )\n    \n    def _extract_unsat_info(self, solver) -> Dict[str, Any]:\n        \"\"\"Extract unsat core and related information for counterexamples\"\"\"\n        try:\n            # Get unsat core (solver should already have unsat_core=True set)\n            unsat_core = solver.unsat_core()\n            \n            assertions = []\n            for assertion in unsat_core:\n                assertions.append(str(assertion))\n            \n            # Get all assertions for context\n            all_assertions = []\n            for assertion in solver.assertions():\n                all_assertions.append(str(assertion))\n            \n            analysis = self._analyze_unsat_reason(assertions, all_assertions)\n            \n            return {\n                'unsat_core': assertions,\n                'all_assertions': all_assertions,\n                'analysis': analysis,\n                'core_size': len(assertions),\n                'total_assertions': len(all_assertions)\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Could not extract unsat core: {str(e)}\")\n            # Try to get at least the assertions\n            try:\n                assertions = [str(a) for a in solver.assertions()]\n                return {\n                    'all_assertions': assertions,\n                    'error': 'Could not extract unsat core (may not be enabled)',\n                    'reason': str(e),\n                    'analysis': 'Enable unsat_core tracking before calling check()'\n                }\n            except:\n                return {\n                    'error': 'Could not extract any information',\n                    'reason': str(e)\n                }\n    \n    def _analyze_unsat_reason(self, unsat_core: List[str], all_assertions: List[str]) -> str:\n        \"\"\"Analyze unsat core to provide human-readable reasons\"\"\"\n        if not unsat_core:\n            return \"No specific unsat core available (possibly not enabled)\"\n        \n        analysis_parts = []\n        core_text = ' '.join(unsat_core).lower()\n        \n        # Count conflicting elements\n        if len(unsat_core) == len(all_assertions):\n            analysis_parts.append(\"All constraints are involved in the conflict\")\n        else:\n            analysis_parts.append(f\"Conflict involves {len(unsat_core)} out of {len(all_assertions)} constraints\")\n        \n        # Identify conflict patterns\n        if 'implies' in core_text and ('false' in core_text or 'not' in core_text):\n            analysis_parts.append(\"Contradiction in implication chain\")\n        if 'and' in core_text and 'not' in core_text:\n            analysis_parts.append(\"Direct contradiction between assertions\")\n        if 'forall' in core_text and 'exists' in core_text:\n            analysis_parts.append(\"Conflict between universal and existential quantifiers\")\n        if 'xor' in core_text:\n            analysis_parts.append(\"Exclusive OR conditions cannot be simultaneously satisfied\")\n        \n        if len(analysis_parts) == 1:\n            analysis_parts.append(\"General logical contradiction in constraints\")\n        \n        return \"; \".join(analysis_parts)\n\n\n# Test function\ndef test_single_formula():\n    api_key = \"AIzaSyCCJvm8lwqFKS-c0sszXXknyQgVK1GSRHU\"\n    \n    if not api_key:\n        raise ValueError(\"API key not provided\")\n    \n    checker = EnterpriseFOLChecker(api_key)\n    \n    # Test with your original formula\n    formula = \"∀x∀y∀z ((System(x) ∧ Customer(y) ∧ RiskProfile(z))→ GuaranteesAnnualizedReturn(x, y, z, 10))∧∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10))∧∀x (HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x))∧∀x (HighRiskCustomer(x) → Customer(x))∧∀x ((GuaranteedPortfolioReturn(x, 10) → PortfolioReturn(x) = 10))∧∀x ((PortfolioReturnLessThan10PercentPerAnnum(x) → PortfolioReturn(x) < 10))∧∃x (HighRiskCustomer(x))\"\n    \n    print(f\"\\nTesting formula: {formula}\")\n    print(\"=\" * 70)\n    \n    result = checker.check_satisfiability(formula)\n    \n    print(f\"\\nStatus: {result.status}\")\n    print(f\"Processing Time: {result.processing_time:.3f}s\")\n    print(\"-\" * 70)\n    \n    if result.status == \"SAT\":\n        print(\"Formula is SATISFIABLE\")\n        if result.model:\n            print(\"\\nModel:\")\n            print(result.model)\n    elif result.status == \"UNSAT\":\n        print(\"Formula is UNSATISFIABLE\")\n        if result.counterexample:\n            print(\"\\n=== UNSAT Analysis ===\")\n            print(f\"Reason: {result.counterexample.get('analysis', 'Unknown')}\")\n            print(f\"\\nCore Size: {result.counterexample.get('core_size', 0)} constraint(s)\")\n            print(f\"Total Constraints: {result.counterexample.get('total_assertions', 0)}\")\n            \n            if result.counterexample.get('unsat_core'):\n                print(\"\\n=== Conflicting Constraints (Unsat Core) ===\")\n                for i, constraint in enumerate(result.counterexample['unsat_core'], 1):\n                    print(f\"{i}. {constraint}\")\n            \n            if result.counterexample.get('all_assertions'):\n                print(\"\\n=== All Constraints ===\")\n                for i, constraint in enumerate(result.counterexample['all_assertions'], 1):\n                    print(f\"{i}. {constraint}\")\n    elif result.status == \"ERROR\":\n        print(f\"Error: {result.error_message}\")\n        if result.z3_code:\n            print(\"\\n=== Generated Z3 Code ===\")\n            print(result.z3_code)\n    elif result.status == \"UNKNOWN\":\n        print(\"Unable to determine satisfiability\")\n    \n    print(\"=\" * 70)\n\nif __name__ == \"__main__\":\n    test_single_formula()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:24:45.357619Z","iopub.execute_input":"2025-10-16T06:24:45.358225Z","iopub.status.idle":"2025-10-16T06:25:03.355557Z","shell.execute_reply.started":"2025-10-16T06:24:45.358202Z","shell.execute_reply":"2025-10-16T06:25:03.354831Z"}},"outputs":[{"name":"stdout","text":"\nTesting formula: ∀x∀y∀z ((System(x) ∧ Customer(y) ∧ RiskProfile(z))→ GuaranteesAnnualizedReturn(x, y, z, 10))∧∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10))∧∀x (HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x))∧∀x (HighRiskCustomer(x) → Customer(x))∧∀x ((GuaranteedPortfolioReturn(x, 10) → PortfolioReturn(x) = 10))∧∀x ((PortfolioReturnLessThan10PercentPerAnnum(x) → PortfolioReturn(x) < 10))∧∃x (HighRiskCustomer(x))\n======================================================================\nUNSAT\n\nStatus: UNSAT\nProcessing Time: 17.964s\n----------------------------------------------------------------------\nFormula is UNSATISFIABLE\n\n=== UNSAT Analysis ===\nReason: Conflict involves 6 out of 7 constraints; General logical contradiction in constraints\n\nCore Size: 6 constraint(s)\nTotal Constraints: 7\n\n=== Conflicting Constraints (Unsat Core) ===\n1. rule2\n2. rule3\n3. rule4\n4. rule5\n5. rule6\n6. rule7\n\n=== All Constraints ===\n1. Implies(rule1,\n        ForAll([x, y, z],\n               Implies(And(System(x),\n                           Customer(y),\n                           RiskProfile(z)),\n                       GuaranteesAnnualizedReturn(x,\n                                        y,\n                                        z,\n                                        10))))\n2. Implies(rule2,\n        ForAll(x,\n               Implies(Customer(x),\n                       GuaranteedPortfolioReturn(x, 10))))\n3. Implies(rule3,\n        ForAll(x,\n               Implies(HighRiskCustomer(x),\n                       PortfolioReturnLessThan10PercentPerAnnum(x))))\n4. Implies(rule4,\n        ForAll(x, Implies(HighRiskCustomer(x), Customer(x))))\n5. Implies(rule5,\n        ForAll(x,\n               Implies(GuaranteedPortfolioReturn(x, 10),\n                       PortfolioReturn(x) == 10)))\n6. Implies(rule6,\n        ForAll(x,\n               Implies(PortfolioReturnLessThan10PercentPerAnnum(x),\n                       PortfolioReturn(x) < 10)))\n7. Implies(rule7, Exists(x, HighRiskCustomer(x)))\n======================================================================\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Example JSON for testing","metadata":{}},{"cell_type":"code","source":"user_stories_data= [\n  {\n    \"story_id\": \"US-001\",\n    \"title\": \"Customer Secure Authentication\",\n    \"description\": \"System must validate customer credentials to provide secure access to financial information.\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-001\",\n        \"criteria\": \"A customer can navigate to the application login page.\",\n        \"related_entities\": [\"customer\", \"authentication_module\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-002\",\n        \"criteria\": \"The system must present input fields for username and password.\",\n        \"related_entities\": [\"system\", \"authentication_module\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-003\",\n        \"criteria\": \"A customer can submit their credentials for verification.\",\n        \"related_entities\": [\"customer\", \"authentication_module\"],\n        \"temporal_order\": 3\n      },\n      {\n        \"ac_id\": \"AC-004\",\n        \"criteria\": \"The system must validate submitted credentials against stored customer records.\",\n        \"related_entities\": [\"system\", \"authentication_module\"],\n        \"temporal_order\": 4\n      },\n      {\n        \"ac_id\": \"AC-005\",\n        \"criteria\": \"If credentials are valid, then the system must grant the customer access to the application dashboard.\",\n        \"related_entities\": [\"system\", \"authentication_module\"],\n        \"temporal_order\": 5\n      },\n      {\n        \"ac_id\": \"AC-006\",\n        \"criteria\": \"If credentials are not valid, then the system must display an authentication error message.\",\n        \"related_entities\": [\"system\", \"authentication_module\"],\n        \"temporal_order\": 5\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-001\", \"AC-002\", \"AC-003\", \"AC-004\", \"AC-005\", \"AC-006\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-002\",\n        \"follows\": \"AC-001\"\n      },\n      {\n        \"precedes\": \"AC-003\",\n        \"follows\": \"AC-002\"\n      },\n      {\n        \"precedes\": \"AC-004\",\n        \"follows\": \"AC-003\"\n      },\n      {\n        \"precedes\": \"AC-005\",\n        \"follows\": \"AC-004\"\n      },\n      {\n        \"precedes\": \"AC-006\",\n        \"follows\": \"AC-004\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-002\",\n    \"title\": \"New Customer Onboarding and Profile Creation\",\n    \"description\": \"System must capture customer details, risk profile, and investment amount to create a complete investment account.\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-007\",\n        \"criteria\": \"A logged-in customer can access the profile creation page.\",\n        \"related_entities\": [\"customer\", \"customer_profile_module\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-008\",\n        \"criteria\": \"The system must present a form to capture required personal details.\",\n        \"related_entities\": [\"system\", \"customer_profile_module\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-009\",\n        \"criteria\": \"The system must present a questionnaire to determine the customer's risk profile.\",\n        \"related_entities\": [\"system\", \"customer_profile_module\"],\n        \"temporal_order\": 3\n      },\n      {\n        \"ac_id\": \"AC-010\",\n        \"criteria\": \"The system must calculate a risk score based on the customer's questionnaire responses.\",\n        \"related_entities\": [\"system\", \"customer_profile_module\"],\n        \"temporal_order\": 4\n      },\n      {\n        \"ac_id\": \"AC-011\",\n        \"criteria\": \"The system must allow the customer to input an initial investment amount.\",\n        \"related_entities\": [\"system\", \"customer_profile_module\"],\n        \"temporal_order\": 5\n      },\n      {\n        \"ac_id\": \"AC-012\",\n        \"criteria\": \"The system must validate that every investment amount is a positive numerical value.\",\n        \"related_entities\": [\"system\", \"customer_profile_module\"],\n        \"temporal_order\": 6\n      },\n      {\n        \"ac_id\": \"AC-013\",\n        \"criteria\": \"Upon submission of all valid information, the system must create a complete customer profile record.\",\n        \"related_entities\": [\"system\", \"customer_profile_module\"],\n        \"temporal_order\": 7\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-007\", \"AC-008\", \"AC-009\", \"AC-010\", \"AC-011\", \"AC-012\", \"AC-013\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-001\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-008\",\n        \"follows\": \"AC-007\"\n      },\n      {\n        \"precedes\": \"AC-009\",\n        \"follows\": \"AC-008\"\n      },\n      {\n        \"precedes\": \"AC-010\",\n        \"follows\": \"AC-009\"\n      },\n      {\n        \"precedes\": \"AC-011\",\n        \"follows\": \"AC-010\"\n      },\n      {\n        \"precedes\": \"AC-012\",\n        \"follows\": \"AC-011\"\n      },\n      {\n        \"precedes\": \"AC-013\",\n        \"follows\": \"AC-012\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-003\",\n    \"title\": \"Automated Ingestion of Real-time Market Data\",\n    \"description\": \"System must automatically fetch and store real-time market data to ensure investment analysis is current.\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-014\",\n        \"criteria\": \"The system must periodically query external sources for Fixed Deposit interest rates.\",\n        \"related_entities\": [\"system\", \"data_analysis_module\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-015\",\n        \"criteria\": \"The system must periodically query external sources for stock prices.\",\n        \"related_entities\": [\"system\", \"data_analysis_module\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-016\",\n        \"criteria\": \"The system must periodically query external sources for mutual fund NAVs.\",\n        \"related_entities\": [\"system\", \"data_analysis_module\"],\n        \"temporal_order\": 3\n      },\n      {\n        \"ac_id\": \"AC-017\",\n        \"criteria\": \"All fetched market data must be stored with a precise timestamp.\",\n        \"related_entities\": [\"system\", \"data_analysis_module\"],\n        \"temporal_order\": 4\n      },\n      {\n        \"ac_id\": \"AC-018\",\n        \"criteria\": \"If any external data source is unavailable, then the system must log an error and attempt a retry.\",\n        \"related_entities\": [\"system\", \"data_analysis_module\"],\n        \"temporal_order\": 5\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-014\", \"AC-015\", \"AC-016\", \"AC-017\", \"AC-018\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-015\",\n        \"follows\": \"AC-014\"\n      },\n      {\n        \"precedes\": \"AC-016\",\n        \"follows\": \"AC-015\"\n      },\n      {\n        \"precedes\": \"AC-017\",\n        \"follows\": \"AC-016\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-004\",\n    \"title\": \"Manager Views Algorithmic Investment Suggestions\",\n    \"description\": \"System must analyze pooled funds and market data to provide algorithmic investment suggestions for managers.\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-019\",\n        \"criteria\": \"A manager can access the investment decision dashboard.\",\n        \"related_entities\": [\"manager\", \"investment_module\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-020\",\n        \"criteria\": \"The system must display the total amount of pooled customer investments available for allocation.\",\n        \"related_entities\": [\"system\", \"investment_module\", \"portfolio_module\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-021\",\n        \"criteria\": \"The system must analyze all available investment options using the latest fetched market data.\",\n        \"related_entities\": [\"system\", \"investment_module\", \"data_analysis_module\"],\n        \"temporal_order\": 3\n      },\n      {\n        \"ac_id\": \"AC-022\",\n        \"criteria\": \"The system must generate a list of suggested investments ranked by potential return.\",\n        \"related_entities\": [\"system\", \"investment_module\"],\n        \"temporal_order\": 4\n      },\n      {\n        \"ac_id\": \"AC-023\",\n        \"criteria\": \"Every investment suggestion must include the investment type, name, expected return, and an associated risk level.\",\n        \"related_entities\": [\"system\", \"investment_module\"],\n        \"temporal_order\": 5\n      },\n      {\n        \"ac_id\": \"AC-024\",\n        \"criteria\": \"Every investment analysis must calculate potential outcomes against a 10 percent target return rate.\",\n        \"related_entities\": [\"system\", \"investment_module\"],\n        \"temporal_order\": 6\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-019\", \"AC-020\", \"AC-021\", \"AC-022\", \"AC-023\", \"AC-024\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-001\", \"US-002\", \"US-003\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-020\",\n        \"follows\": \"AC-019\"\n      },\n      {\n        \"precedes\": \"AC-021\",\n        \"follows\": \"AC-020\"\n      },\n      {\n        \"precedes\": \"AC-022\",\n        \"follows\": \"AC-021\"\n      },\n      {\n        \"precedes\": \"AC-023\",\n        \"follows\": \"AC-022\"\n      },\n      {\n        \"precedes\": \"AC-024\",\n        \"follows\": \"AC-023\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-005\",\n    \"title\": \"Customer Views Real-time Investment Portfolio\",\n    \"description\": \"System must display a customer's real-time portfolio value and performance to allow for investment tracking.\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-025\",\n        \"criteria\": \"A customer can navigate to their portfolio dashboard after a successful login.\",\n        \"related_entities\": [\"customer\", \"portfolio_module\", \"authentication_module\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-026\",\n        \"criteria\": \"The system must display the customer's total initial investment amount.\",\n        \"related_entities\": [\"system\", \"portfolio_module\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-027\",\n        \"criteria\": \"The system must display a detailed breakdown of all investments allocated to the customer.\",\n        \"related_entities\": [\"system\", \"portfolio_module\"],\n        \"temporal_order\": 3\n      },\n      {\n        \"ac_id\": \"AC-028\",\n        \"criteria\": \"For each investment, the system must show the current market value based on the latest data.\",\n        \"related_entities\": [\"system\", \"portfolio_module\", \"data_analysis_module\"],\n        \"temporal_order\": 4\n      },\n      {\n        \"ac_id\": \"AC-029\",\n        \"criteria\": \"The system must display the overall portfolio gain or loss as a percentage.\",\n        \"related_entities\": [\"system\", \"portfolio_module\"],\n        \"temporal_order\": 5\n      },\n      {\n        \"ac_id\": \"AC-030\",\n        \"criteria\": \"All portfolio data must be updated using the most recent market data available in the system.\",\n        \"related_entities\": [\"system\", \"portfolio_module\"],\n        \"temporal_order\": 6\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-025\", \"AC-026\", \"AC-027\", \"AC-028\", \"AC-029\", \"AC-030\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-001\", \"US-003\", \"US-004\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-026\",\n        \"follows\": \"AC-025\"\n      },\n      {\n        \"precedes\": \"AC-027\",\n        \"follows\": \"AC-026\"\n      },\n      {\n        \"precedes\": \"AC-028\",\n        \"follows\": \"AC-027\"\n      },\n      {\n        \"precedes\": \"AC-029\",\n        \"follows\": \"AC-028\"\n      },\n      {\n        \"precedes\": \"AC-030\",\n        \"follows\": \"AC-029\"\n      }\n    ]\n  }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.489176Z","iopub.status.idle":"2025-10-16T06:23:46.489499Z","shell.execute_reply.started":"2025-10-16T06:23:46.489337Z","shell.execute_reply":"2025-10-16T06:23:46.489352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport json\nfrom typing import Dict, List, Any\nimport copy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.491869Z","iopub.status.idle":"2025-10-16T06:23:46.492115Z","shell.execute_reply.started":"2025-10-16T06:23:46.492016Z","shell.execute_reply":"2025-10-16T06:23:46.492026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_stories_data =[\n  {\n    \"story_id\": \"US-001\",\n    \"title\": \"Customer Registration and Risk Profile Assessment\",\n    \"description\": \"System must authenticate customers securely and capture their risk profile including investment preferences, time horizon, and risk tolerance to enable personalized portfolio recommendations\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-001\",\n        \"criteria\": \"System must allow all customers to register with basic KYC details within 5 minutes\",\n        \"related_entities\": [\"Customer\", \"RegistrationModule\", \"KYCModule\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-002\",\n        \"criteria\": \"System must prevent registration completion until full KYC verification is completed which requires 24-48 hours\",\n        \"related_entities\": [\"Customer\", \"RegistrationModule\", \"KYCModule\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-003\",\n        \"criteria\": \"System must capture risk profile immediately after registration before any investment\",\n        \"related_entities\": [\"Customer\", \"RiskProfileModule\"],\n        \"temporal_order\": 3\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-001\", \"AC-002\", \"AC-003\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-002\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-002\",\n        \"follows\": \"AC-001\"\n      },\n      {\n        \"precedes\": \"AC-003\",\n        \"follows\": \"AC-002\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-002\",\n    \"title\": \"Real-time Portfolio Management and Updates\",\n    \"description\": \"System must maintain each customer portfolio with real-time market data updates and provide instantaneous valuation changes to customers and fund manager\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-004\",\n        \"criteria\": \"System must update portfolio valuations every second based on real-time market feeds\",\n        \"related_entities\": [\"Portfolio\", \"MarketDataModule\", \"ValuationEngine\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-005\",\n        \"criteria\": \"System must batch portfolio updates every 15 minutes to prevent system overload\",\n        \"related_entities\": [\"Portfolio\", \"ValuationEngine\", \"DatabaseModule\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-006\",\n        \"criteria\": \"System must display portfolio changes immediately to customers upon any transaction\",\n        \"related_entities\": [\"Portfolio\", \"CustomerInterface\", \"TransactionModule\"],\n        \"temporal_order\": 2\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-004\", \"AC-005\", \"AC-006\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-003\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-006\",\n        \"follows\": \"AC-004\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-003\",\n    \"title\": \"Investment Decision Algorithmic Recommendations\",\n    \"description\": \"System must analyze investment options including fixed deposits, stocks, and mutual funds by checking real-time web data and provide algorithmic suggestions to fund manager targeting 10% annual returns\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-007\",\n        \"criteria\": \"System must guarantee minimum 10% annual returns for all customer investments\",\n        \"related_entities\": [\"Portfolio\", \"ReturnCalculationModule\", \"Customer\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-008\",\n        \"criteria\": \"System must only recommend investments with historical returns above 8% and risk rating below moderate\",\n        \"related_entities\": [\"InvestmentOption\", \"RecommendationEngine\", \"RiskModule\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-009\",\n        \"criteria\": \"System must prevent any investment recommendation during market volatility exceeding 3% daily fluctuation\",\n        \"related_entities\": [\"MarketDataModule\", \"RecommendationEngine\", \"VolatilityCalculator\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-010\",\n        \"criteria\": \"System must make investment recommendations every hour regardless of market conditions\",\n        \"related_entities\": [\"RecommendationEngine\", \"SchedulerModule\"],\n        \"temporal_order\": 3\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-007\", \"AC-008\", \"AC-009\", \"AC-010\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-002\", \"US-004\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-008\",\n        \"follows\": \"AC-007\"\n      },\n      {\n        \"precedes\": \"AC-010\",\n        \"follows\": \"AC-008\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-004\",\n    \"title\": \"Web Data Scraping for Investment Analysis\",\n    \"description\": \"System must automatically check web sources for FD interest rates across banks, real-time stock prices, and mutual fund NAVs to support investment decision-making\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-011\",\n        \"criteria\": \"System must scrape financial data from at least 50 different web sources simultaneously every 5 minutes\",\n        \"related_entities\": [\"WebScraperModule\", \"DataSource\", \"SchedulerModule\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-012\",\n        \"criteria\": \"System must comply with robots.txt and rate limiting requiring minimum 30 second intervals between requests to same source\",\n        \"related_entities\": [\"WebScraperModule\", \"ComplianceModule\", \"DataSource\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-013\",\n        \"criteria\": \"System must use only official API sources and must not perform web scraping for regulatory compliance\",\n        \"related_entities\": [\"APIModule\", \"ComplianceModule\", \"DataSource\"],\n        \"temporal_order\": 1\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-011\", \"AC-012\", \"AC-013\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-003\"],\n    \"temporal_constraints\": []\n  },\n  {\n    \"story_id\": \"US-005\",\n    \"title\": \"Manager Investment Pooling and Execution\",\n    \"description\": \"System must enable fund manager to pool customer investments and execute trades across fixed deposits, stocks, and mutual funds while maintaining individual customer portfolio allocations\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-014\",\n        \"criteria\": \"System must allow manager to execute pooled trades of any size without customer approval\",\n        \"related_entities\": [\"Manager\", \"TradeExecutionModule\", \"Customer\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-015\",\n        \"criteria\": \"System must require explicit customer approval for any transaction exceeding 10% of their portfolio value\",\n        \"related_entities\": [\"Customer\", \"TradeExecutionModule\", \"ApprovalModule\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-016\",\n        \"criteria\": \"System must maintain separate audit trail for each customer transaction within pooled investments\",\n        \"related_entities\": [\"Customer\", \"AuditModule\", \"TradeExecutionModule\"],\n        \"temporal_order\": 2\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-014\", \"AC-015\", \"AC-016\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-001\", \"US-002\", \"US-003\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-016\",\n        \"follows\": \"AC-014\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-006\",\n    \"title\": \"Customer Login and Authentication\",\n    \"description\": \"System must provide secure multi-factor authentication for customer login with session management and password policies\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-017\",\n        \"criteria\": \"System must allow customers to login within 10 seconds using single-factor password authentication\",\n        \"related_entities\": [\"Customer\", \"AuthenticationModule\", \"LoginInterface\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-018\",\n        \"criteria\": \"System must enforce mandatory two-factor authentication for all customer logins including OTP verification\",\n        \"related_entities\": [\"Customer\", \"AuthenticationModule\", \"OTPService\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-019\",\n        \"criteria\": \"System must allow offline access to portfolio data without internet connectivity\",\n        \"related_entities\": [\"Customer\", \"PortfolioModule\", \"CacheModule\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-020\",\n        \"criteria\": \"System must require continuous internet connection for all security-critical operations including login\",\n        \"related_entities\": [\"Customer\", \"AuthenticationModule\", \"NetworkModule\"],\n        \"temporal_order\": 1\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-017\", \"AC-018\", \"AC-019\", \"AC-020\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-001\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-019\",\n        \"follows\": \"AC-017\"\n      }\n    ]\n  },\n  {\n    \"story_id\": \"US-007\",\n    \"title\": \"Investment Amount Capture and Validation\",\n    \"description\": \"System must capture customer investment amounts with validation rules ensuring compliance with minimum investment requirements and available balance\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-021\",\n        \"criteria\": \"System must allow customers to invest any amount from 100 rupees to unlimited\",\n        \"related_entities\": [\"Customer\", \"InvestmentModule\", \"ValidationEngine\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-022\",\n        \"criteria\": \"System must enforce minimum investment amount of 50000 rupees per transaction\",\n        \"related_entities\": [\"Customer\", \"InvestmentModule\", \"ValidationEngine\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-023\",\n        \"criteria\": \"System must cap maximum single transaction at 10000 rupees to prevent money laundering\",\n        \"related_entities\": [\"Customer\", \"ComplianceModule\", \"InvestmentModule\"],\n        \"temporal_order\": 1\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-021\", \"AC-022\", \"AC-023\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-001\", \"US-005\"],\n    \"temporal_constraints\": []\n  },\n  {\n    \"story_id\": \"US-008\",\n    \"title\": \"Performance and Scalability Requirements\",\n    \"description\": \"System must handle multiple concurrent users with low latency and high throughput while maintaining data consistency\",\n    \"acceptance_criteria\": [\n      {\n        \"ac_id\": \"AC-024\",\n        \"criteria\": \"System must support unlimited concurrent users with zero performance degradation\",\n        \"related_entities\": [\"ServerInfrastructure\", \"LoadBalancer\", \"DatabaseCluster\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-025\",\n        \"criteria\": \"System must be deployed on single server infrastructure with maximum 100 GB storage\",\n        \"related_entities\": [\"ServerInfrastructure\", \"DeploymentModule\"],\n        \"temporal_order\": 1\n      },\n      {\n        \"ac_id\": \"AC-026\",\n        \"criteria\": \"System must process each transaction with database ACID guarantees and immediate consistency\",\n        \"related_entities\": [\"DatabaseModule\", \"TransactionProcessor\"],\n        \"temporal_order\": 2\n      },\n      {\n        \"ac_id\": \"AC-027\",\n        \"criteria\": \"System must use eventual consistency model to achieve sub-millisecond response times\",\n        \"related_entities\": [\"DatabaseModule\", \"CacheLayer\"],\n        \"temporal_order\": 2\n      }\n    ],\n    \"compounded_ac\": {\n      \"ac_ids\": [\"AC-024\", \"AC-025\", \"AC-026\", \"AC-027\"]\n    },\n    \"story_level_check\": [\"compounded_ac\", \"description\"],\n    \"cross_story_dependencies\": [\"US-002\", \"US-005\"],\n    \"temporal_constraints\": [\n      {\n        \"precedes\": \"AC-026\",\n        \"follows\": \"AC-024\"\n      }\n    ]\n  }\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.493043Z","iopub.status.idle":"2025-10-16T06:23:46.493393Z","shell.execute_reply.started":"2025-10-16T06:23:46.493196Z","shell.execute_reply":"2025-10-16T06:23:46.493210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"updated_stories = copy.deepcopy(user_stories_data)\nupdated_stories","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.494448Z","iopub.status.idle":"2025-10-16T06:23:46.494719Z","shell.execute_reply.started":"2025-10-16T06:23:46.494577Z","shell.execute_reply":"2025-10-16T06:23:46.494612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"updated_stories[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.495423Z","iopub.status.idle":"2025-10-16T06:23:46.495683Z","shell.execute_reply.started":"2025-10-16T06:23:46.495543Z","shell.execute_reply":"2025-10-16T06:23:46.495553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.496745Z","iopub.status.idle":"2025-10-16T06:23:46.497031Z","shell.execute_reply.started":"2025-10-16T06:23:46.496903Z","shell.execute_reply":"2025-10-16T06:23:46.496917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.498287Z","iopub.status.idle":"2025-10-16T06:23:46.498572Z","shell.execute_reply.started":"2025-10-16T06:23:46.498406Z","shell.execute_reply":"2025-10-16T06:23:46.498418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for story in tqdm(updated_stories):\n    data_point = {'NL': story['description']}\n    print(\"NL: \", story['description'])\n    full_resp_str, resp_parts = simple_generate(input_str=data_point)\n    story['description_fol'] = resp_parts[1]\n    print ( \"FOL: \", resp_parts[1])\n    \n    for ac in tqdm(story['acceptance_criteria']):\n        data_point = {'NL': ac['criteria']}\n        print(\"NL: \", ac['criteria'])\n        full_resp_str, resp_parts = simple_generate(input_str=data_point)\n        ac['criteria_fol'] = resp_parts[1]\n        print ( \"FOL: \", resp_parts[1])\n        \nwith open('user_stories_with_fol.json', 'w', encoding='utf-8') as f:\n    json.dump(updated_stories, f, indent=2, ensure_ascii=False)\n\nprint(\"Done. Saved to user_stories_with_fol.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.499895Z","iopub.status.idle":"2025-10-16T06:23:46.500182Z","shell.execute_reply.started":"2025-10-16T06:23:46.500020Z","shell.execute_reply":"2025-10-16T06:23:46.500036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open('user_stories_with_fol.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n\n# Pretty print with symbols\nprint(json.dumps(data, indent=2, ensure_ascii=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.500780Z","iopub.status.idle":"2025-10-16T06:23:46.501352Z","shell.execute_reply.started":"2025-10-16T06:23:46.501172Z","shell.execute_reply":"2025-10-16T06:23:46.501187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing for single module verification","metadata":{}},{"cell_type":"code","source":"import json\nfrom typing import Dict, Any, List\n\n# Assuming EnterpriseFOLChecker is defined as per your shared code\n\nclass FOLDriver:\n    def __init__(self, api_key: str):\n        self.checker = EnterpriseFOLChecker(api_key)\n        self.result_map: Dict[str, List[Dict[str, Any]]] = {}\n\n    def run_check(self, story_json: List[Dict[str, Any]]) -> Dict[str, Any]:\n        for story in story_json:\n            story_id = story[\"story_id\"]\n            description_fol = story[\"description_fol\"]\n\n            print(f\"\\n\\n=== Processing Story: {story_id} ===\")\n            ac_formulas = []\n\n            for ac in story[\"acceptance_criteria\"]:\n                ac_id = ac[\"ac_id\"]\n                formula = ac[\"criteria_fol\"]\n\n                print(f\"\\n[Checking AC] {story_id} :: {ac_id}\")\n                result = self.checker.check_satisfiability(formula)\n\n                # Store detailed info per AC\n                entry = {\n                    \"ac_id\": ac_id,\n                    \"formula\": formula,\n                    \"status\": result.status,\n                    \"processing_time\": result.processing_time,\n                }\n\n                if result.status == \"UNSAT\":\n                    entry[\"counterexample\"] = result.counterexample\n                    print(f\" ❌ UNSAT :: {ac_id} | Reason: {result.counterexample.get('analysis', 'Unknown')}\")\n                elif result.status == \"SAT\":\n                    ac_formulas.append(formula)\n                    print(f\" ✅ SAT :: {ac_id}\")\n                    if result.model:\n                        entry[\"model\"] = str(result.model)\n                elif result.status in [\"ERROR\", \"UNKNOWN\"]:\n                    entry[\"error_message\"] = result.error_message if result.status == \"ERROR\" else \"Satisfiability unknown\"\n                    print(f\" ⚠️ {result.status} :: {ac_id} | Reason: {entry['error_message']}\")\n\n                self.result_map.setdefault(story_id, []).append(entry)\n\n            # Check compounded ACs if at least one SAT\n            if ac_formulas:\n                compounded_formula = \" ∧ \".join(f\"({f})\" for f in ac_formulas)\n                print(f\"\\n[Checking Compounded AC] {story_id}\")\n                result = self.checker.check_satisfiability(compounded_formula)\n\n                entry = {\n                    \"ac_id\": \"compounded_ac\",\n                    \"formula\": compounded_formula,\n                    \"status\": result.status,\n                    \"processing_time\": result.processing_time,\n                }\n\n                if result.status == \"UNSAT\":\n                    entry[\"counterexample\"] = result.counterexample\n                    print(f\" ❌ UNSAT :: Compounded AC | Reason: {result.counterexample.get('analysis', 'Unknown')}\")\n                elif result.status == \"SAT\":\n                    print(f\" ✅ SAT :: Compounded AC\")\n                    if result.model:\n                        entry[\"model\"] = str(result.model)\n                else:\n                    entry[\"error_message\"] = result.error_message if result.status == \"ERROR\" else \"Satisfiability unknown\"\n                    print(f\" ⚠️ {result.status} :: Compounded AC | Reason: {entry.get('error_message')}\")\n\n                self.result_map.setdefault(story_id, []).append(entry)\n\n                # Story-level consistency\n                story_level_formula = f\"({compounded_formula}) ∧ ({description_fol})\"\n                print(f\"\\n[Checking Story-Level Consistency] {story_id}\")\n                result = self.checker.check_satisfiability(story_level_formula)\n\n                entry = {\n                    \"ac_id\": \"story_level\",\n                    \"formula\": story_level_formula,\n                    \"status\": result.status,\n                    \"processing_time\": result.processing_time,\n                }\n\n                if result.status == \"UNSAT\":\n                    entry[\"counterexample\"] = result.counterexample\n                    print(f\" ❌ UNSAT :: Story-Level | Reason: {result.counterexample.get('analysis', 'Unknown')}\")\n                elif result.status == \"SAT\":\n                    print(f\" ✅ SAT :: Story-Level\")\n                    if result.model:\n                        entry[\"model\"] = str(result.model)\n                else:\n                    entry[\"error_message\"] = result.error_message if result.status == \"ERROR\" else \"Satisfiability unknown\"\n                    print(f\" ⚠️ {result.status} :: Story-Level | Reason: {entry.get('error_message')}\")\n\n                self.result_map.setdefault(story_id, []).append(entry)\n\n        return self.result_map\n\n\nif __name__ == \"__main__\":\n    with open(\"user_stories_with_fol.json\", \"r\") as f:\n        stories_data = json.load(f)\n\n    api_key = \"GOOGLE_API_KEY\"\n    driver = FOLDriver(api_key)\n    result_map = driver.run_check(stories_data)\n\n    print(\"\\n=== Final Result Map ===\")\n    print(json.dumps(result_map, indent=2, ensure_ascii=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.502747Z","iopub.status.idle":"2025-10-16T06:23:46.503021Z","shell.execute_reply.started":"2025-10-16T06:23:46.502876Z","shell.execute_reply":"2025-10-16T06:23:46.502890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ndef extract_acs_per_entities(stories_json):\n    \"\"\"\n    Extract acceptance criteria per entity across current story and its cross-story dependencies.\n\n    Args:\n        stories_json (list): List of story dictionaries as per your schema.\n\n    Returns:\n        dict: Mapping { entity_name : [list of AC ids across cross stories and current story] }\n    \"\"\"\n    # Map stories by story_id for quick lookup\n    stories_by_id = {story[\"story_id\"]: story for story in stories_json}\n\n    # Result map\n    entity_to_ac_ids = defaultdict(list)\n\n    # Process each story\n    for story in stories_json:\n        current_story_id = story[\"story_id\"]\n        # Collect ACs of current story\n        current_acs = story.get(\"acceptance_criteria\", [])\n\n        # Collect dependent stories\n        dependencies = story.get(\"cross_story_dependencies\", [])\n\n        # Gather ACs from dependent stories\n        dep_acs = []\n        for dep_id in dependencies:\n            dep_story = stories_by_id.get(dep_id)\n            if dep_story:\n                dep_acs.extend(dep_story.get(\"acceptance_criteria\", []))\n\n        # Combine current + dependent ACs\n        combined_acs = current_acs + dep_acs\n\n        # For each AC in current story, check for overlapping entities with dependent ACs\n        for ac in current_acs:\n            ac_entities = set(ac.get(\"related_entities\", []))\n            if not ac_entities:\n                continue\n\n            # For each entity in AC, add AC id\n            for entity in ac_entities:\n                # Include current AC\n                if ac[\"ac_id\"] not in entity_to_ac_ids[entity]:\n                    entity_to_ac_ids[entity].append(ac[\"ac_id\"])\n\n                # Include dependent ACs that overlap in entity\n                for dep_ac in dep_acs:\n                    dep_entities = set(dep_ac.get(\"related_entities\", []))\n                    if entity in dep_entities and dep_ac[\"ac_id\"] not in entity_to_ac_ids[entity]:\n                        entity_to_ac_ids[entity].append(dep_ac[\"ac_id\"])\n\n    return dict(entity_to_ac_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.503967Z","iopub.status.idle":"2025-10-16T06:23:46.504206Z","shell.execute_reply.started":"2025-10-16T06:23:46.504105Z","shell.execute_reply":"2025-10-16T06:23:46.504115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print( extract_acs_per_entities(stories_data) )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.504978Z","iopub.status.idle":"2025-10-16T06:23:46.505265Z","shell.execute_reply.started":"2025-10-16T06:23:46.505096Z","shell.execute_reply":"2025-10-16T06:23:46.505112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stories_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.506548Z","iopub.status.idle":"2025-10-16T06:23:46.506798Z","shell.execute_reply.started":"2025-10-16T06:23:46.506696Z","shell.execute_reply":"2025-10-16T06:23:46.506706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cross_story_entity_verifier.py\nimport json\nimport re\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Any\n\nfrom z3 import (\n    DeclareSort,\n    Function,\n    BoolSort,\n    IntSort,\n    Const,\n    IntVal,\n    ForAll,\n    Exists,\n    And,\n    Or,\n    Not,\n    Implies,\n    Solver,\n    sat,\n    unsat,\n)\n\n# ---------------------------\n# Helper: extract_acs_per_entities (from you)\n# ---------------------------\ndef extract_acs_per_entities(stories_json):\n    stories_by_id = {story[\"story_id\"]: story for story in stories_json}\n    entity_to_ac_ids = defaultdict(list)\n\n    for story in stories_json:\n        current_story_id = story[\"story_id\"]\n        current_acs = story.get(\"acceptance_criteria\", [])\n        dependencies = story.get(\"cross_story_dependencies\", [])\n\n        dep_acs = []\n        for dep_id in dependencies:\n            dep_story = stories_by_id.get(dep_id)\n            if dep_story:\n                dep_acs.extend(dep_story.get(\"acceptance_criteria\", []))\n\n        combined_acs = current_acs + dep_acs\n\n        for ac in current_acs:\n            ac_entities = set(ac.get(\"related_entities\", []))\n            if not ac_entities:\n                continue\n            for entity in ac_entities:\n                if ac[\"ac_id\"] not in entity_to_ac_ids[entity]:\n                    entity_to_ac_ids[entity].append(ac[\"ac_id\"])\n                for dep_ac in dep_acs:\n                    dep_entities = set(dep_ac.get(\"related_entities\", []))\n                    if entity in dep_entities and dep_ac[\"ac_id\"] not in entity_to_ac_ids[entity]:\n                        entity_to_ac_ids[entity].append(dep_ac[\"ac_id\"])\n\n    return dict(entity_to_ac_ids)\n\n\n# ---------------------------\n# Utilities to map AC ids -> formulas (from stories JSON)\n# ---------------------------\ndef build_acid_to_formula_map(stories_json: List[Dict[str, Any]]) -> Dict[str, str]:\n    m = {}\n    for story in stories_json:\n        for ac in story.get(\"acceptance_criteria\", []):\n            ac_id = ac[\"ac_id\"]\n            formula = ac.get(\"criteria_fol\") or ac.get(\"formula\")  # tolerant keys\n            if formula:\n                m[ac_id] = formula\n    return m\n\n\n# ---------------------------\n# Simple predicate signature collector (decides arg sorts: U or Int)\n# ---------------------------\ndef collect_predicate_signatures(formulas: List[str]) -> Dict[str, List[str]]:\n    \"\"\"\n    Return mapping: predicate_name -> list of arg sorts ('U'|'Int')\n    \"\"\"\n    pred_sigs: Dict[str, List[str]] = {}\n    patt = re.compile(r\"([A-Za-z_][A-Za-z0-9_]*)\\s*\\(([^()]+)\\)\")\n    for f in formulas:\n        for match in patt.finditer(f):\n            pred = match.group(1)\n            args_str = match.group(2)\n            args = [a.strip() for a in args_str.split(\",\")]\n            arity = len(args)\n            prev = pred_sigs.get(pred)\n            # initialize if needed\n            if prev is None:\n                pred_sigs[pred] = []\n            # Ensure list length\n            while len(pred_sigs[pred]) < arity:\n                pred_sigs[pred].append(\"U\")\n            for i, arg in enumerate(args):\n                if re.fullmatch(r\"\\d+\", arg):\n                    pred_sigs[pred][i] = \"Int\"  # numeric constant -> Int argument\n                # else keep 'U' (object) by default\n    return pred_sigs\n\n\n# ---------------------------\n# Z3 infrastructure creation\n# ---------------------------\ndef create_z3_predicates(pred_sigs: Dict[str, List[str]], u_sort):\n    pred_funcs = {}\n    for name, args in pred_sigs.items():\n        sorts = []\n        for a in args:\n            sorts.append(IntSort() if a == \"Int\" else u_sort)\n        pred_funcs[name] = Function(name, *sorts, BoolSort())\n    return pred_funcs\n\n\n# ---------------------------\n# Tokenizer + recursive parser for the subset of FOL we need\n# ---------------------------\n\n# token kinds: NAME, NUMBER, LPAREN, RPAREN, COMMA, AND, OR, IMPLIES, NOT, FORALL, EXISTS\n_token_spec = [\n    (\"SKIP\", r\"[ \\t\\n\\r]+\"),\n    (\"FORALL\", r\"∀\"),\n    (\"EXISTS\", r\"∃\"),\n    (\"AND\", r\"∧\"),\n    (\"OR\", r\"∨\"),\n    (\"IMPLIES\", r\"→|->\"),\n    (\"NOT\", r\"¬|~\"),\n    (\"LPAREN\", r\"\\(\"),\n    (\"RPAREN\", r\"\\)\"),\n    (\"COMMA\", r\",\"),\n    (\"NUMBER\", r\"\\d+\"),\n    (\"NAME\", r\"[A-Za-z_][A-Za-z0-9_]*\"),\n    (\"OTHER\", r\".\"),\n]\n_token_re = re.compile(\"|\".join(\"(?P<%s>%s)\" % pair for pair in _token_spec))\n\n\ndef tokenize(s: str):\n    tokens = []\n    pos = 0\n    while pos < len(s):\n        m = _token_re.match(s, pos)\n        if not m:\n            raise SyntaxError(f\"Unexpected input at {pos}: {s[pos:]}\")\n        kind = m.lastgroup\n        val = m.group()\n        pos = m.end()\n        if kind == \"SKIP\":\n            continue\n        if kind == \"OTHER\":\n            # attempt to tolerate non-ascii spaces or other small chars by skipping\n            continue\n        tokens.append((kind, val))\n    return tokens\n\n\nclass Parser:\n    def __init__(self, tokens, pred_funcs, u_sort):\n        self.tokens = tokens\n        self.i = 0\n        self.pred_funcs = pred_funcs\n        self.u_sort = u_sort\n        self.const_cache = {}  # constants not bound by quantifiers: name -> z3.Const\n        self.var_env_stack = []  # stack of dicts for variable scopes\n\n    def peek(self):\n        return self.tokens[self.i] if self.i < len(self.tokens) else (None, None)\n\n    def consume(self, expected_kind=None):\n        if self.i >= len(self.tokens):\n            raise SyntaxError(\"Unexpected end of tokens\")\n        tok = self.tokens[self.i]\n        if expected_kind and tok[0] != expected_kind:\n            raise SyntaxError(f\"Expected {expected_kind}, got {tok}\")\n        self.i += 1\n        return tok\n\n    def parse(self):\n        return self.parse_implies()\n\n    def parse_implies(self):\n        left = self.parse_or()\n        tok = self.peek()\n        if tok[0] == \"IMPLIES\":\n            self.consume(\"IMPLIES\")\n            right = self.parse_implies()\n            return Implies(left, right)\n        return left\n\n    def parse_or(self):\n        left = self.parse_and()\n        while self.peek()[0] == \"OR\":\n            self.consume(\"OR\")\n            right = self.parse_and()\n            left = Or(left, right)\n        return left\n\n    def parse_and(self):\n        left = self.parse_not()\n        while self.peek()[0] == \"AND\":\n            self.consume(\"AND\")\n            right = self.parse_not()\n            left = And(left, right)\n        return left\n\n    def parse_not(self):\n        if self.peek()[0] == \"NOT\":\n            self.consume(\"NOT\")\n            expr = self.parse_not()\n            return Not(expr)\n        return self.parse_atom()\n\n    def _current_var_env(self):\n        if self.var_env_stack:\n            return self.var_env_stack[-1]\n        return {}\n\n    def parse_atom(self):\n        tok_kind, tok_val = self.peek()\n        if tok_kind is None:\n            raise SyntaxError(\"Unexpected end while parsing atom\")\n\n        # Parenthesized expression\n        if tok_kind == \"LPAREN\":\n            self.consume(\"LPAREN\")\n            expr = self.parse_implies()\n            if self.peek()[0] != \"RPAREN\":\n                raise SyntaxError(\"Expected closing ')'\")\n            self.consume(\"RPAREN\")\n            return expr\n\n        # Quantifier inside the expression\n        if tok_kind in (\"FORALL\", \"EXISTS\"):\n            q_kind = tok_kind\n            self.consume(q_kind)\n            # next token must be a NAME (variable)\n            vtok = self.consume(\"NAME\")\n            varname = vtok[1]\n            # create z3 variable for quantifier\n            z3_var = Const(varname, self.u_sort)\n            # push a new var env with this variable\n            new_env = dict(self._current_var_env())\n            new_env[varname] = z3_var\n            self.var_env_stack.append(new_env)\n            # parse quantifier's body (could be parenthesized or immediate atom)\n            body = self.parse_atom()\n            # pop var env\n            self.var_env_stack.pop()\n            if q_kind == \"FORALL\":\n                return ForAll([z3_var], body)\n            else:\n                return Exists([z3_var], body)\n\n        # Predicate application or bare name\n        if tok_kind == \"NAME\":\n            # lookahead for LPAREN -> predicate call\n            # But also allow chained quantifiers like \"∀x∀y ( ... )\" - handled as repeated FORALL tokens earlier\n            if (self.i + 1) < len(self.tokens) and self.tokens[self.i + 1][0] == \"LPAREN\":\n                # predicate call\n                name = self.consume(\"NAME\")[1]\n                self.consume(\"LPAREN\")\n                args = []\n                while True:\n                    a_kind, a_val = self.peek()\n                    if a_kind == \"NUMBER\":\n                        self.consume(\"NUMBER\")\n                        args.append((\"NUMBER\", a_val))\n                    elif a_kind == \"NAME\":\n                        self.consume(\"NAME\")\n                        args.append((\"NAME\", a_val))\n                    else:\n                        # empty arg? not expected\n                        raise SyntaxError(f\"Unexpected predicate arg token: {a_kind} {a_val}\")\n                    if self.peek()[0] == \"COMMA\":\n                        self.consume(\"COMMA\")\n                        continue\n                    break\n                if self.peek()[0] != \"RPAREN\":\n                    raise SyntaxError(\"Expected ')' after predicate args\")\n                self.consume(\"RPAREN\")\n                # Evaluate predicate application to a z3 BoolRef\n                # Map args to z3 terms\n                mapped_args = []\n                current_env = self._current_var_env()\n                for kind, val in args:\n                    if kind == \"NUMBER\":\n                        mapped_args.append(IntVal(int(val)))\n                    else:  # NAME\n                        # if var bound in current var env -> z3 Const of U sort\n                        if val in current_env:\n                            mapped_args.append(current_env[val])\n                        else:\n                            # treat as a constant in domain\n                            if val not in self.const_cache:\n                                self.const_cache[val] = Const(val, self.u_sort)\n                            mapped_args.append(self.const_cache[val])\n                if name not in self.pred_funcs:\n                    raise SyntaxError(f\"Predicate '{name}' unknown for this parser run\")\n                return self.pred_funcs[name](*mapped_args)\n            else:\n                # NAME alone (treat as boolean constant or variable) - not expected often\n                name = self.consume(\"NAME\")[1]\n                current_env = self._current_var_env()\n                if name in current_env:\n                    return current_env[name]\n                if name not in self.const_cache:\n                    self.const_cache[name] = Const(name, self.u_sort)\n                return self.const_cache[name]\n\n        raise SyntaxError(f\"Unexpected token in atom: {tok_kind} '{tok_val}'\")\n\n\ndef parse_fol_to_z3(formula: str, pred_funcs: Dict[str, Any], u_sort):\n    \"\"\"\n    Parse a single FOL formula string (limited grammar) and return a z3 BoolRef.\n    \"\"\"\n    tokens = tokenize(formula)\n    parser = Parser(tokens, pred_funcs, u_sort)\n    expr = parser.parse()\n    # If tokens remain, we might have had leading quantifiers like ∀x∀y <body>. Parser allows only one top-level parse,\n    # but if more tokens remain it's a syntax risk.\n    if parser.i != len(tokens):\n        # attempt to parse any trailing quantifiers (rare). For safety raise.\n        raise SyntaxError(\"Extra tokens after parsing formula: \" + str(tokens[parser.i :]))\n    return expr\n\n\n# ---------------------------\n# Report structure and main verification routine\n# ---------------------------\ndef find_ac_result(result_map: Dict[str, List[Dict[str, Any]]], ac_id: str):\n    for story_id, entries in result_map.items():\n        for entry in entries:\n            if entry.get(\"ac_id\") == ac_id:\n                return entry\n    return None\n\n\ndef entity_level_cross_story_verifier(stories_data: List[Dict[str, Any]], result_map: Dict[str, Any]):\n    \"\"\"\n    Main routine. Returns a structured report as dictionary and writes a JSON file 'cross_story_entity_report.json'.\n    \"\"\"\n    # 1) build ac_id -> formula lookup from stories data\n    ac_to_formula = build_acid_to_formula_map(stories_data)\n\n    # 2) compute entity -> ac ids map (uses your function)\n    entity_to_ac_ids = extract_acs_per_entities(stories_data)\n\n    # 3) prepare all formulas (for predicate signature collection)\n    all_formulas = list(ac_to_formula.values())\n\n    # Also consider story-level 'description_fol' formulas if present (they may be needed),\n    # include them to ensure predicates defined there are recognized.\n    for s in stories_data:\n        if s.get(\"description_fol\"):\n            all_formulas.append(s[\"description_fol\"])\n\n    # 4) collect predicate signatures and create z3 predicate functions & a universal domain sort\n    U = DeclareSort(\"U\")\n    pred_sigs = collect_predicate_signatures(all_formulas)\n    pred_funcs = create_z3_predicates(pred_sigs, U)\n\n    # Prepare report skeleton\n    report = {\n        \"entities\": {},\n        \"cross_entity_compound\": None,\n        \"summary\": {\"entities_processed\": 0, \"entities_unsat_individual\": 0, \"entities_compounded_unsat\": 0},\n    }\n\n    # For later: store entity compound z3 expressions (only SAT ones)\n    entity_compound_exprs = {}\n\n    # 5) For each entity, check each AC id's status using result_map\n    for entity, ac_ids in entity_to_ac_ids.items():\n        report[\"summary\"][\"entities_processed\"] += 1\n        entity_entry = {\"entity\": entity, \"individual_ac_checks\": [], \"compound\": None}\n        entity_unsat = False\n        unsat_reasons = []\n\n        # iterate AC ids\n        for acid in ac_ids:\n            res = find_ac_result(result_map, acid)\n            ac_info = {\"ac_id\": acid, \"found_in_result_map\": bool(res)}\n            if not res:\n                ac_info[\"status\"] = \"MISSING\"\n                ac_info[\"reason\"] = \"AC id not found in provided result_map\"\n                entity_unsat = True\n                unsat_reasons.append({\"ac_id\": acid, \"reason\": \"missing in result_map\"})\n            else:\n                ac_info[\"status\"] = res.get(\"status\")\n                ac_info[\"processing_time\"] = res.get(\"processing_time\")\n                if res.get(\"status\") == \"UNSAT\":\n                    # capture counterexample if present\n                    entity_unsat = True\n                    entity_unsat_ac = {\"ac_id\": acid, \"counterexample\": res.get(\"counterexample\")}\n                    ac_info[\"counterexample\"] = res.get(\"counterexample\")\n                    unsat_reasons.append(entity_unsat_ac)\n                elif res.get(\"status\") in (\"ERROR\", \"UNKNOWN\"):\n                    entity_unsat = True\n                    ac_info[\"reason\"] = res.get(\"error_message\", \"Unknown / checker error\")\n                    unsat_reasons.append({\"ac_id\": acid, \"reason\": ac_info[\"reason\"]})\n                else:\n                    # SAT case: collect model string (if available) for record\n                    if res.get(\"model\"):\n                        ac_info[\"model_text\"] = res.get(\"model\")\n            entity_entry[\"individual_ac_checks\"].append(ac_info)\n\n        # if any individual AC unsat -> entity-level unsat (report and skip compound z3 check)\n        if entity_unsat:\n            report[\"summary\"][\"entities_unsat_individual\"] += 1\n            entity_entry[\"compound\"] = {\n                \"status\": \"UNSAT_DUE_TO_INDIVIDUAL_AC\",\n                \"unsat_reasons\": unsat_reasons,\n                \"compound_formula\": None,\n                \"z3_model\": None,\n            }\n            report[\"entities\"][entity] = entity_entry\n            continue\n\n        # 6) All individual ACs SAT -> build compound formula (conjunction of AC formulas for these ac_ids)\n        formulas = []\n        missing_formula_acids = []\n        for acid in ac_ids:\n            f = ac_to_formula.get(acid)\n            if not f:\n                missing_formula_acids.append(acid)\n            else:\n                formulas.append(f)\n        if missing_formula_acids:\n            # Can't compose if some formulas are missing\n            entity_entry[\"compound\"] = {\n                \"status\": \"ERROR\",\n                \"reason\": f\"Missing formulas for AC ids: {missing_formula_acids}\",\n                \"compound_formula\": None,\n                \"z3_model\": None,\n            }\n            report[\"entities\"][entity] = entity_entry\n            continue\n\n        # Parse each formula into z3 and create a conjunction\n        try:\n            parsed_exprs = []\n            for f in formulas:\n                expr = parse_fol_to_z3(f, pred_funcs, U)\n                parsed_exprs.append(expr)\n            compound_expr = And(*parsed_exprs) if len(parsed_exprs) > 1 else parsed_exprs[0]\n\n            # Check with Z3\n            s = Solver()\n            s.add(compound_expr)\n            check = s.check()\n            if check == sat:\n                m = s.model()\n                entity_entry[\"compound\"] = {\n                    \"status\": \"SAT\",\n                    \"compound_formula\": \" ∧ \".join(formulas),\n                    \"z3_model\": str(m),\n                }\n                # keep parsed expression for eventual cross-entity compound\n                entity_compound_exprs[entity] = compound_expr\n            elif check == unsat:\n                report[\"summary\"][\"entities_compounded_unsat\"] += 1\n                entity_entry[\"compound\"] = {\n                    \"status\": \"UNSAT\",\n                    \"compound_formula\": \" ∧ \".join(formulas),\n                    \"z3_model\": None,\n                }\n            else:\n                entity_entry[\"compound\"] = {\n                    \"status\": \"UNKNOWN\",\n                    \"compound_formula\": \" ∧ \".join(formulas),\n                    \"z3_model\": None,\n                }\n        except Exception as ex:\n            entity_entry[\"compound\"] = {\n                \"status\": \"ERROR_PARSING_OR_SOLVING\",\n                \"reason\": str(ex),\n                \"compound_formula\": \" ∧ \".join(formulas),\n                \"z3_model\": None,\n            }\n\n        report[\"entities\"][entity] = entity_entry\n\n    # 7) After all entity-level checks, build cross-entity compound of all entity compounds (only those that are SAT)\n    cross_entities = list(entity_compound_exprs.items())\n    if not cross_entities:\n        report[\"cross_entity_compound\"] = {\n            \"status\": \"NOT_ATTEMPTED\",\n            \"reason\": \"No entity-level compounds were SAT and available for cross-entity check.\",\n            \"z3_model\": None,\n        }\n    else:\n        all_exprs = [expr for (e, expr) in cross_entities]\n        combined = And(*all_exprs) if len(all_exprs) > 1 else all_exprs[0]\n        s = Solver()\n        s.add(combined)\n        chk = s.check()\n        if chk == sat:\n            report[\"cross_entity_compound\"] = {\n                \"status\": \"SAT\",\n                \"z3_model\": str(s.model()),\n                \"entities_included\": [e for e, _ in cross_entities],\n            }\n        elif chk == unsat:\n            report[\"cross_entity_compound\"] = {\n                \"status\": \"UNSAT\",\n                \"z3_model\": None,\n                \"entities_included\": [e for e, _ in cross_entities],\n            }\n        else:\n            report[\"cross_entity_compound\"] = {\n                \"status\": \"UNKNOWN\",\n                \"z3_model\": None,\n                \"entities_included\": [e for e, _ in cross_entities],\n            }\n\n    # 8) write report JSON to disk\n    with open(\"cross_story_entity_report.json\", \"w\", encoding=\"utf-8\") as fh:\n        json.dump(report, fh, indent=2, ensure_ascii=False)\n\n    return report\n\n\n# ---------------------------\n# Example usage (main)\n# ---------------------------\nif __name__ == \"__main__\":\n    # Placeholders: load your stories_data and result_map JSON files here\n    # For example:\n    # with open(\"user_stories_with_fol.json\") as f:\n    #     stories_data = json.load(f)\n    # with open(\"result_map.json\") as f:\n    #     result_map = json.load(f)\n\n    # If you already have them in memory, just pass them into the function like:\n    report = entity_level_cross_story_verifier(stories_data, result_map)\n    print(report)\n    # For demonstration purposes this script won't run a demo (no IO) unless you supply inputs.\n    print(\"Module loaded. Call entity_level_cross_story_verifier(stories_data, result_map) with your data.\")\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.508413Z","iopub.status.idle":"2025-10-16T06:23:46.508707Z","shell.execute_reply.started":"2025-10-16T06:23:46.508572Z","shell.execute_reply":"2025-10-16T06:23:46.508601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nprint(json.dumps(report, indent=2, ensure_ascii=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T06:23:46.510150Z","iopub.status.idle":"2025-10-16T06:23:46.510463Z","shell.execute_reply.started":"2025-10-16T06:23:46.510290Z","shell.execute_reply":"2025-10-16T06:23:46.510316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport logging\nimport time\nfrom google.generativeai import types\nfrom typing import Tuple, Optional, Dict, Any, List\nfrom dataclasses import dataclass\nimport google.generativeai as genai\nfrom z3 import *\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SATResult:\n    \"\"\"Enhanced result container for SAT checking\"\"\"\n    status: str  # SAT, UNSAT, UNKNOWN, ERROR\n    formula: str\n    z3_code: str\n    model: Optional[Any] = None\n    counterexample: Optional[Dict[str, Any]] = None\n    error_message: Optional[str] = None\n    processing_time: Optional[float] = None\n\n@dataclass\nclass NLFOLPair:\n    \"\"\"Container for Natural Language and FOL formula pairs\"\"\"\n    natural_language: str\n    fol_formula: str\n    \n    def __repr__(self):\n        return f\"NL: {self.natural_language}\\nFOL: {self.fol_formula}\"\n\n@dataclass\nclass LinkingAxioms:\n    \"\"\"Container for discovered linking axioms\"\"\"\n    axioms: List[str]\n    explanations: List[str]\n    \n    def __repr__(self):\n        result = \"=== Discovered Linking Axioms ===\\n\"\n        for i, (axiom, explanation) in enumerate(zip(self.axioms, self.explanations), 1):\n            result += f\"{i}. {axiom}\\n   Explanation: {explanation}\\n\"\n        return result\n\n\nclass GeminiAxiomDiscovery:\n    \"\"\"Discovers linking axioms from NL-FOL pairs using Gemini\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        genai.configure(api_key=api_key)\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\n        \n        self.discovery_prompt = \"\"\"\nYou are an expert in First-Order Logic and semantic analysis. Your mission is to discover LINKING AXIOMS - the hidden logical bridges between business requirements.\n\n## What Are Linking Axioms?\n\nLinking axioms are IMPLICIT logical relationships that:\n1. Connect predicates across different requirements\n2. Define the semantic meaning of abstract predicates\n3. Establish domain constraints and type hierarchies\n4. Are NEVER explicitly stated but are NECESSARY for logical completeness\n\n## Input Requirements\n\n{nl_fol_pairs}\n\n## Systematic Discovery Process\n\nFollow these steps to discover ALL necessary linking axioms:\n\n### STEP 1: Semantic Predicate Analysis\n\nRead the NATURAL LANGUAGE carefully to understand what each predicate MEANS semantically.\n\nFor each predicate, ask:\n- What real-world concept does this represent?\n- Does it have a specific type (entity, property, relation, comparison)?\n- Does it reference a concrete value or an abstract concept?\n\nExample domains to guide your thinking:\n\n**Healthcare Domain:**\n- NL: \"Patients with chronic conditions must receive quarterly checkups\"\n- FOL: ∀x (ChronicPatient(x) → RequiresQuarterlyCheckup(x))\n- Semantic analysis: ChronicPatient is a TYPE of Patient\n- Linking axiom needed: ∀x (ChronicPatient(x) → Patient(x))\n\n**Access Control Domain:**\n- NL: \"Admin users can modify any record, regular users cannot\"\n- FOL: ∀x (AdminUser(x) → CanModify(x, AllRecords))\n- Semantic analysis: AdminUser is a SPECIALIZED type of User\n- Linking axiom needed: ∀x (AdminUser(x) → User(x))\n\n**Financial Domain:**\n- NL: \"High-value transactions must be approved within 24 hours\"\n- FOL: ∀x (HighValueTransaction(x) → RequiresApprovalWithin24Hours(x))\n- Semantic analysis: \"HighValueTransaction\" is abstract, needs concrete definition\n- Linking axiom needed: ∀x (HighValueTransaction(x) → TransactionValue(x) > 10000)\n\n### STEP 2: Identify Type Hierarchies (Subtype Relationships)\n\nLook for predicates where one is a SPECIALIZED version of another.\n\nPattern recognition:\n- SpecificType + GeneralType → ∀x (SpecificType(x) → GeneralType(x))\n- Examples: PremiumCustomer → Customer, EmergencyCase → MedicalCase, SeniorEmployee → Employee\n\nQuestions to ask:\n- Is this predicate describing a subset of entities described by another predicate?\n- Does the natural language use words like \"type of\", \"kind of\", \"special\", \"specific\"?\n- Would an instance of the specific predicate always be an instance of the general predicate?\n\n### STEP 3: Map Abstract Predicates to Concrete Values\n\nIdentify predicates that express abstract concepts but need concrete mathematical or logical definitions.\n\nPattern recognition:\n- Guarantee/Promise predicates: GuaranteedX(y, value) → ActualX(y) = value\n- Threshold predicates: MeetsThreshold(x) → Metric(x) ≥ threshold_value\n- Status predicates: IsApproved(x) → ApprovalStatus(x) = \"APPROVED\"\n\nExamples across domains:\n\n**E-commerce:**\n- FreeShipping(order) → OrderTotal(order) ≥ 50\n- PriorityDelivery(order) → DeliveryTime(order) ≤ 2\n\n**Education:**\n- PassingGrade(student, course) → Score(student, course) ≥ 60\n- HonorsStudent(x) → GPA(x) ≥ 3.5\n\n**Manufacturing:**\n- MeetsQualityStandard(product) → DefectRate(product) < 0.01\n- UrgentOrder(x) → LeadTime(x) ≤ 24\n\nQuestions to ask:\n- Does this predicate name contain \"Guaranteed\", \"Required\", \"Approved\", \"Qualified\", \"Certified\"?\n- Is there a numeric parameter that suggests a concrete value relationship?\n- What does it ACTUALLY mean for this predicate to be true?\n\n### STEP 4: Convert Comparison Predicates to Operators\n\nIdentify predicates whose NAMES indicate comparisons but don't have mathematical operators.\n\nPattern recognition:\n- LessThan/Below/Under → metric < value\n- GreaterThan/Above/Over → metric > value\n- AtLeast/Minimum → metric ≥ value\n- AtMost/Maximum → metric ≤ value\n- Between → lowerBound ≤ metric ≤ upperBound\n\nExamples:\n\n**Performance Monitoring:**\n- ResponseTimeTooSlow(system) → ResponseTime(system) > 200\n- HighCPUUsage(server) → CPULoad(server) > 80\n\n**Business Rules:**\n- LowInventory(product) → StockLevel(product) < 10\n- ExcessiveOvertime(employee) → WeeklyHours(employee) > 50\n\nQuestions to ask:\n- Does the predicate name contain comparison words?\n- Is there an implied numerical threshold in the natural language?\n- What mathematical relationship does this predicate represent?\n\n### STEP 5: Assert Existence Constraints\n\nIdentify when entities MUST exist for the requirements to be non-vacuous (not trivially true).\n\nPattern recognition:\n- If constraints specifically apply to a specialized type, ensure ∃x SpecializedType(x)\n- If two requirements create tension/conflict, entities must exist to test that conflict\n\nExamples:\n\n**Policy Compliance:**\n- Requirement: \"All sensitive documents must be encrypted\"\n- Requirement: \"Executives can access unencrypted documents\"\n- Linking axiom: ∃x (SensitiveDocument(x)) ∧ ∃y (Executive(y))\n\n**Resource Allocation:**\n- Requirement: \"Critical tasks get priority resources\"\n- Linking axiom: ∃x (CriticalTask(x))\n\nQuestions to ask:\n- Would the requirements be vacuously satisfied if no entities of this type exist?\n- Do multiple requirements reference the same specialized type in different ways?\n- Is there potential for logical tension that needs concrete instances to test?\n\n### STEP 6: Unify Semantically Equivalent Predicates\n\nIdentify different predicates that refer to the SAME underlying property or concept.\n\nPattern recognition:\n- Result predicates and value predicates measuring the same thing\n- Different names for the same measurement\n- Abstract predicates and their concrete implementations\n\nExamples:\n\n**System Performance:**\n- SystemResponsive(x) and ResponseTime(x) < threshold → link them\n- FastProcessing(x) and ProcessingDuration(x) → both measure speed\n\n**Financial Systems:**\n- AccountBalance(x) and CurrentFunds(x) → same concept\n- CreditWorthy(x) and CreditScore(x) ≥ threshold → related concepts\n\nQuestions to ask:\n- Do multiple predicates conceptually measure or describe the same property?\n- Are there abstract and concrete versions of the same concept?\n- Would knowing one predicate tell you about another?\n\n## Critical Rules for Output\n\n1. **Think semantically**: Understand what each predicate MEANS in the real world\n2. **Use the natural language**: The NL text contains semantic clues about relationships\n3. **Generate complete FOL**: Every axiom must be syntactically valid FOL\n4. **No input repetition**: Do NOT output the original formulas\n5. **Cover all 6 steps**: Systematically check each discovery pattern\n\n## Output Format\n\nReturn ONLY a JSON array:\n[\n  {{\"axiom\": \"∀x (SpecificType(x) → GeneralType(x))\", \"explanation\": \"Type hierarchy: SpecificType is a subtype of GeneralType based on NL semantic analysis\"}},\n  {{\"axiom\": \"∀x (AbstractPredicate(x, 10) → ConcreteMeasure(x) = 10)\", \"explanation\": \"Maps abstract guarantee to concrete numeric value\"}},\n  ...\n]\n\nValid FOL operators: ∀ ∃ → ∧ ∨ ¬ = < > ≤ ≥\n\nStart with [ and end with ]. No markdown formatting. No code blocks. Pure JSON only.\n\nNow analyze the input requirements above step-by-step and output your discovered linking axioms.\n\"\"\"\n    \n    def discover_linking_axioms(self, nl_fol_pairs: List[NLFOLPair]) -> LinkingAxioms:\n        \"\"\"Discover linking axioms from NL-FOL pairs\"\"\"\n        try:\n            print(\"\\n\" + \"=\"*70)\n            print(\"STEP 1: DISCOVERING LINKING AXIOMS\")\n            print(\"=\"*70)\n            \n            # Format input pairs with both NL and FOL\n            pairs_text = \"\"\n            for i, pair in enumerate(nl_fol_pairs, 1):\n                pairs_text += f\"\\nRequirement {i}:\\n\"\n                pairs_text += f\"Natural Language: {pair.natural_language}\\n\"\n                pairs_text += f\"FOL Formula: {pair.fol_formula}\\n\"\n            \n            print(f\"\\nInput NL-FOL Pairs:\\n{pairs_text}\")\n            \n            prompt = self.discovery_prompt.format(nl_fol_pairs=pairs_text)\n            \n            generation_config = genai.types.GenerationConfig(\n                max_output_tokens=12000,\n                temperature=0.4,  # Allow creative reasoning\n                top_p=0.95,\n                top_k=40,\n                candidate_count=1\n            )\n            \n            print(\"\\n[DEBUG] Calling Gemini API for axiom discovery...\")\n            print(f\"[DEBUG] Prompt length: {len(prompt)} chars\")\n            \n            response = self.model.generate_content(prompt, generation_config=generation_config)\n            \n            if not response.text:\n                print(\"[ERROR] Empty response from Gemini API\")\n                raise ValueError(\"Empty response from Gemini API\")\n            \n            print(f\"\\n[DEBUG] ========== RAW GEMINI RESPONSE START ==========\")\n            print(response.text)\n            print(f\"[DEBUG] ========== RAW GEMINI RESPONSE END ==========\")\n            print(f\"[DEBUG] Response length: {len(response.text)} characters\\n\")\n            \n            # Parse JSON response\n            linking_axioms = self._parse_axiom_response(response.text)\n            \n            print(f\"\\n[DEBUG] Parsing result:\")\n            print(f\"  - Number of axioms extracted: {len(linking_axioms.axioms)}\")\n            \n            if len(linking_axioms.axioms) == 0:\n                print(\"\\n[WARNING] ⚠️  ZERO AXIOMS EXTRACTED!\")\n                print(\"[ERROR] LLM failed to discover linking axioms\")\n                print(\"[DEBUG] Check the raw response above to see what went wrong\")\n            else:\n                print(f\"\\n[SUCCESS] Discovered {len(linking_axioms.axioms)} linking axioms\")\n                print(linking_axioms)\n            \n            return linking_axioms\n            \n        except Exception as e:\n            logger.error(f\"Axiom discovery failed: {str(e)}\")\n            print(f\"\\n[ERROR] Exception during axiom discovery: {str(e)}\")\n            return LinkingAxioms(axioms=[], explanations=[])\n    \n    def _parse_axiom_response(self, response: str) -> LinkingAxioms:\n        \"\"\"Parse JSON response to extract linking axioms\"\"\"\n        import json\n        \n        print(\"\\n[DEBUG] Starting JSON parsing...\")\n        \n        cleaned = response.strip()\n        \n        # Remove markdown\n        # cleaned = re.sub(r'```\n        cleaned = re.sub(r'```\\s*', '', cleaned)\n        \n        # Extract JSON array\n        json_match = re.search(r'\\[.*\\]', cleaned, re.DOTALL)\n        if json_match:\n            cleaned = json_match.group(0)\n            print(f\"[DEBUG] Extracted JSON array (length: {len(cleaned)} chars)\")\n        else:\n            print(\"[ERROR] ❌ No JSON array found in response\")\n            return LinkingAxioms(axioms=[], explanations=[])\n        \n        try:\n            axioms_data = json.loads(cleaned)\n            print(f\"[DEBUG] ✓ JSON parsed successfully\")\n            print(f\"[DEBUG] Found {len(axioms_data) if isinstance(axioms_data, list) else 0} items\")\n            \n            if not isinstance(axioms_data, list):\n                return LinkingAxioms(axioms=[], explanations=[])\n            \n            axioms = []\n            explanations = []\n            \n            for i, item in enumerate(axioms_data):\n                if isinstance(item, dict) and 'axiom' in item:\n                    axioms.append(item['axiom'].strip())\n                    explanations.append(item.get('explanation', '').strip())\n                    print(f\"  ✓ Axiom {i+1}: {item['axiom'][:70]}...\")\n            \n            return LinkingAxioms(axioms=axioms, explanations=explanations)\n            \n        except json.JSONDecodeError as e:\n            print(f\"[ERROR] ❌ JSON parsing failed: {str(e)}\")\n            return LinkingAxioms(axioms=[], explanations=[])\n\n    def _fallback_parse_axioms(self, response: str) -> LinkingAxioms:\n        \"\"\"Fallback parser using regex to extract axioms\"\"\"\n        print(\"\\n[DEBUG] Using fallback regex parser...\")\n        \n        # Look for FOL patterns with quantifiers and logical operators\n        # We attempt to capture chunks that start with ∀ or ∃ and contain common logical connectors\n        fol_pattern = r'([∀∃][\\s\\S]{0,2000}?(?:→|→|∧|∨|¬|=|<|>|≤|≥)[\\s\\S]{0,2000}?)'\n        matches = re.findall(fol_pattern, response, re.DOTALL)\n        \n        print(f\"[DEBUG] Regex found {len(matches)} potential FOL formulas\")\n        \n        axioms = []\n        for i, match in enumerate(matches):\n            cleaned = match.strip().strip(',').strip('\"').strip(\"'\").strip()\n            # Basic validity: must have quantifier and some operators\n            if len(cleaned) > 5 and any(op in cleaned for op in ['→', '∧', '∨', '=', '<', '>', '≤', '≥', '¬']):\n                # Clean up any trailing punctuation or text\n                cleaned = re.sub(r'[,;.]$', '', cleaned)\n                axioms.append(cleaned)\n                print(f\"  {i+1}. Extracted: {cleaned[:100]}...\")\n            else:\n                print(f\"  {i+1}. Rejected (too short or invalid): {cleaned[:50]}...\")\n        \n        explanations = [\"Extracted via fallback regex parser\"] * len(axioms)\n        \n        print(f\"\\n[DEBUG] Fallback extraction complete: {len(axioms)} axioms\")\n        \n        return LinkingAxioms(axioms=axioms, explanations=explanations)\n\n\nclass FOLCompoundBuilder:\n    \"\"\"Builds compound FOL formula from original formulas and linking axioms\"\"\"\n    \n    @staticmethod\n    def build_compound_formula(nl_fol_pairs: List[NLFOLPair], linking_axioms: LinkingAxioms) -> str:\n        \"\"\"Combine all FOL formulas with linking axioms using conjunction\"\"\"\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"STEP 2: BUILDING COMPOUND FOL FORMULA\")\n        print(\"=\"*70)\n        \n        all_formulas = []\n        \n        # Add original FOL formulas\n        print(\"\\n[DEBUG] Adding original FOL formulas:\")\n        for i, pair in enumerate(nl_fol_pairs, 1):\n            formula = pair.fol_formula.strip()\n            all_formulas.append(formula)\n            print(f\"  {i}. {formula}\")\n        \n        print(f\"\\n[DEBUG] Original formulas count: {len(nl_fol_pairs)}\")\n        print(f\"[DEBUG] Linking axioms count: {len(linking_axioms.axioms)}\")\n        \n        # Add linking axioms\n        if len(linking_axioms.axioms) > 0:\n            print(\"\\n[DEBUG] Adding linking axioms:\")\n            for i, axiom in enumerate(linking_axioms.axioms, 1):\n                axiom_clean = axiom.strip()\n                all_formulas.append(axiom_clean)\n                print(f\"  {i}. {axiom_clean}\")\n        else:\n            print(\"\\n[WARNING] No linking axioms to add!\")\n        \n        print(f\"\\n[DEBUG] Total formulas before compounding: {len(all_formulas)}\")\n        \n        # Join with conjunction (∧)\n        compound_formula = \" ∧ \".join(all_formulas)\n        \n        print(f\"\\n[SUCCESS] Compound formula created with {len(all_formulas)} total formulas\")\n        print(f\"  - Original specifications: {len(nl_fol_pairs)}\")\n        print(f\"  - Linking axioms: {len(linking_axioms.axioms)}\")\n        print(f\"\\nCompound Formula (first 500 chars):\")\n        print(\"-\" * 70)\n        print(compound_formula[:500] + (\"...\" if len(compound_formula) > 500 else \"\"))\n        print(\"-\" * 70)\n        print(f\"Full formula length: {len(compound_formula)} characters\\n\")\n        \n        return compound_formula\n\n\nclass GeminiFlashTranslator:\n    \"\"\"Handles translation of FOL to Z3 using Gemini Flash 2.5\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        genai.configure(api_key=api_key)\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\n        \n        self.translation_prompt = \"\"\"\nCRITICAL: Convert this First-Order Logic formula to executable Z3 Python code. \nOUTPUT ONLY RAW PYTHON CODE - NO EXPLANATIONS, NO MARKDOWN.\n\nFORMULA: {fol_formula}\n\nREQUIREMENTS:\n1. Import: from z3 import *\n2. Create solver with unsat_core enabled: s = Solver()\n   s.set(unsat_core=True)\n3. Define all variables as Int or Bool\n4. Define predicates as Function(name, IntSort(), BoolSort())\n5. Add formulas using assert_and_track with unique labels:\n   s.assert_and_track(formula1, \"rule1\")\n   s.assert_and_track(formula2, \"rule2\")\n6. Check satisfiability: result = s.check()\n7. For quantifiers: Use ForAll([x], ...) and Exists([x], ...)\n8. Map operators: → to Implies, ∧ to And, ∨ to Or, ¬ to Not, ⊕ to Xor, ↔ to ==\n\nIMPORTANT: Use this exact structure:\nfrom z3 import *\ns = Solver()\ns.set(unsat_core=True)\n\n# Define variables and predicates\nx = Int('x')\ny = Int('y')\nz = Int('z')\nCreatedRepo = Function('CreatedRepo', IntSort(), BoolSort())\nHuman = Function('Human', IntSort(), BoolSort())\n\n# Add formulas with tracking\ns.assert_and_track(ForAll([x], Implies(CreatedRepo(x), Xor(Human(x), Alien(x)))), \"rule1\")\n\n# Check satisfiability\nresult = s.check()\nif result == sat:\n    model = s.model()\nelif result == unsat:\n    print(\"UNSAT\")\nelse:\n    print(\"UNKNOWN\")\n\nCODE:\n\"\"\"\n    \n    def translate_to_z3(self, fol_formula: str) -> Tuple[Optional[str], Optional[str]]:\n        \"\"\"Translate FOL to Z3 code using Gemini Flash 2.5\"\"\"\n        try:\n            print(\"\\n\" + \"=\"*70)\n            print(\"STEP 3: TRANSLATING FOL TO Z3 CODE\")\n            print(\"=\"*70)\n            \n            prompt = self.translation_prompt.format(fol_formula=fol_formula)\n\n            generation_config = genai.types.GenerationConfig(\n                max_output_tokens=20000, \n                temperature=0.1, \n                top_p=0.8, \n                top_k=40,\n                candidate_count=1\n            )\n            \n            print(\"\\n[DEBUG] Calling Gemini API for Z3 translation...\")\n            response = self.model.generate_content(prompt, generation_config=generation_config)\n            \n            if not getattr(response, \"text\", None):\n                raise ValueError(\"Empty response from Gemini API\")\n            \n            print(f\"\\n[DEBUG] Raw translation response received ({len(response.text)} chars)\")\n            \n            z3_code = self._clean_gemini_response(response.text)\n            z3_code = self._ensure_unsat_tracking(z3_code)\n            \n            if self._validate_z3_code(z3_code):\n                print(\"\\n[SUCCESS] Z3 code validation passed\")\n                return z3_code, None\n            else:\n                print(\"\\n[WARNING] Z3 code validation failed, attempting repair...\")\n                repaired_code = self._repair_z3_code(z3_code, fol_formula)\n                if self._validate_z3_code(repaired_code):\n                    print(\"[SUCCESS] Z3 code repaired successfully\")\n                    return repaired_code, None\n                return None, \"Generated Z3 code failed validation and repair\"\n                \n        except Exception as e:\n            logger.error(f\"Gemini translation failed: {str(e)}\")\n            return None, f\"Translation error: {str(e)}\"\n    \n    def _clean_gemini_response(self, response: str) -> str:\n        \"\"\"Clean and extract Z3 code from Gemini response\"\"\"\n        cleaned = response\n        # Remove triple-backtick blocks\n        cleaned = re.sub(r'```(?:python)?\\n', '', cleaned)\n        cleaned = re.sub(r'```', '', cleaned)\n        cleaned = cleaned.strip().strip('\"').strip(\"'\")\n        \n        # Ensure from z3 import present\n        if \"from z3 import *\" not in cleaned:\n            cleaned = \"from z3 import *\\n\" + cleaned\n            \n        return cleaned\n    \n    def _ensure_unsat_tracking(self, code: str) -> str:\n        \"\"\"Ensure unsat_core tracking is enabled in the solver\"\"\"\n        if \"s.set(unsat_core=True)\" not in code:\n            if \"s = Solver()\" in code:\n                code = code.replace(\"s = Solver()\", \"s = Solver()\\ns.set(unsat_core=True)\")\n            else:\n                code = \"s = Solver()\\ns.set(unsat_core=True)\\n\" + code\n        return code\n    \n    def _repair_z3_code(self, code: str, original_formula: str) -> str:\n        \"\"\"Attempt to repair common issues in generated Z3 code\"\"\"\n        repaired = code\n        \n        # Attempt to find single-letter variables\n        variables = set(re.findall(r'\\b([a-z])\\b', original_formula))\n        # Attempt to find predicates that start with capital letter\n        predicates = set(re.findall(r'([A-Z][a-zA-Z0-9_]*)\\b', original_formula))\n        \n        for var in variables:\n            if f\"{var} = Int('{var}')\" not in repaired and f\"{var} = Int(\" not in repaired:\n                repaired = f\"{var} = Int('{var}')\\n\" + repaired\n        \n        for pred in predicates:\n            if f\"{pred} = Function('{pred}'\" not in repaired:\n                repaired = f\"{pred} = Function('{pred}', IntSort(), BoolSort())\\n\" + repaired\n        \n        # Replace s.add(...) with s.assert_and_track(..., \"ruleN\")\n        rule_counter = 1\n        def add_to_track(match):\n            nonlocal rule_counter\n            inner = match.group(1).strip()\n            replacement = f's.assert_and_track({inner}, \"rule{rule_counter}\")'\n            rule_counter += 1\n            return replacement\n        \n        repaired = re.sub(r's\\.add\\(\\s*(.*?)\\s*\\)', add_to_track, repaired, flags=re.DOTALL)\n        \n        repaired = self._ensure_unsat_tracking(repaired)\n        \n        if \"s.check()\" not in repaired and \"result = s.check()\" not in repaired:\n            repaired += \"\\nresult = s.check()\\n\"\n            \n        return repaired\n    \n    def _validate_z3_code(self, code: str) -> bool:\n        \"\"\"Basic validation of generated Z3 code\"\"\"\n        required_elements = [\"from z3 import\", \"Solver()\"]\n        return all(elem in code for elem in required_elements)\n\n\nclass EnterpriseFOLChecker:\n    \"\"\"Enterprise-grade FOL satisfiability checker with Gemini integration\"\"\"\n    \n    def __init__(self, gemini_api_key: str):\n        self.translator = GeminiFlashTranslator(gemini_api_key)\n        \n    def check_satisfiability(self, fol_formula: str) -> SATResult:\n        \"\"\"Main method to check FOL satisfiability with comprehensive error handling\"\"\"\n        start_time = time.time()\n        \n        try:\n            print(\"\\n\" + \"=\"*70)\n            print(\"STEP 4: CHECKING SATISFIABILITY WITH Z3\")\n            print(\"=\"*70)\n            \n            z3_code, error = self.translator.translate_to_z3(fol_formula)\n            \n            if error or z3_code is None:\n                return SATResult(\n                    status=\"ERROR\",\n                    formula=fol_formula,\n                    z3_code=\"\" if z3_code is None else z3_code,\n                    error_message=f\"Translation failed: {error}\",\n                    processing_time=time.time() - start_time\n                )\n            \n            print(\"\\n[DEBUG] Executing Z3 code...\")\n            result = self._execute_z3_code(z3_code, fol_formula)\n            result.processing_time = time.time() - start_time\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Unexpected error during SAT checking: {str(e)}\")\n            return SATResult(\n                status=\"ERROR\",\n                formula=fol_formula,\n                z3_code=\"\",\n                error_message=f\"Runtime error: {str(e)}\",\n                processing_time=time.time() - start_time\n            )\n    \n    def _execute_z3_code(self, z3_code: str, original_formula: str) -> SATResult:\n        \"\"\"Execute Z3 code in a proper execution environment\"\"\"\n        try:\n            local_vars = {}\n            \n            from z3 import (Solver, Int, Bool, Function, ForAll, Exists, Implies, \n                          And, Or, Not, Xor, sat, unsat, unknown, IntSort, BoolSort)\n            \n            safe_builtins = {\n                'print': print, 'str': str, 'int': int, 'bool': bool,\n                'list': list, 'dict': dict, 'range': range, 'len': len,\n            }\n            \n            local_vars.update({\n                'Solver': Solver, 'Int': Int, 'Bool': Bool, 'Function': Function,\n                'ForAll': ForAll, 'Exists': Exists, 'Implies': Implies,\n                'And': And, 'Or': Or, 'Not': Not, 'Xor': Xor,\n                'sat': sat, 'unsat': unsat, 'unknown': unknown,\n                'IntSort': IntSort, 'BoolSort': BoolSort,\n            })\n            local_vars.update(safe_builtins)\n            \n            # Execute generated Z3 code in controlled locals\n            exec(z3_code, local_vars, local_vars)\n            \n            s = local_vars.get('s')\n            result = local_vars.get('result')\n            model = local_vars.get('model')\n            \n            if not s:\n                return SATResult(\n                    status=\"ERROR\",\n                    formula=original_formula,\n                    z3_code=z3_code,\n                    error_message=\"Solver not found in generated code\"\n                )\n            \n            if result is None:\n                result = s.check()\n            \n            if result == sat:\n                if model is None:\n                    model = s.model()\n                print(\"\\n[RESULT] Formula is SATISFIABLE\")\n                return SATResult(\n                    status=\"SAT\",\n                    formula=original_formula,\n                    z3_code=z3_code,\n                    model=model\n                )\n                \n            elif result == unsat:\n                print(\"\\n[RESULT] Formula is UNSATISFIABLE\")\n                counterexample = self._extract_unsat_info(s)\n                return SATResult(\n                    status=\"UNSAT\",\n                    formula=original_formula,\n                    z3_code=z3_code,\n                    counterexample=counterexample\n                )\n                \n            else:\n                print(\"\\n[RESULT] Satisfiability UNKNOWN\")\n                return SATResult(\n                    status=\"UNKNOWN\",\n                    formula=original_formula,\n                    z3_code=z3_code\n                )\n                \n        except Exception as e:\n            logger.error(f\"Z3 execution error: {str(e)}\")\n            return SATResult(\n                status=\"ERROR\",\n                formula=original_formula,\n                z3_code=z3_code,\n                error_message=f\"Z3 execution error: {str(e)}\"\n            )\n    \n    def _extract_unsat_info(self, solver) -> Dict[str, Any]:\n        \"\"\"Extract unsat core and related information for counterexamples\"\"\"\n        try:\n            unsat_core = solver.unsat_core()\n            \n            assertions = [str(assertion) for assertion in unsat_core]\n            all_assertions = [str(assertion) for assertion in solver.assertions()]\n            \n            analysis = self._analyze_unsat_reason(assertions, all_assertions)\n            \n            return {\n                'unsat_core': assertions,\n                'all_assertions': all_assertions,\n                'analysis': analysis,\n                'core_size': len(assertions),\n                'total_assertions': len(all_assertions)\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Could not extract unsat core: {str(e)}\")\n            try:\n                assertions = [str(a) for a in solver.assertions()]\n                return {\n                    'all_assertions': assertions,\n                    'error': 'Could not extract unsat core',\n                    'reason': str(e),\n                    'analysis': 'Enable unsat_core tracking before calling check()'\n                }\n            except:\n                return {'error': 'Could not extract any information', 'reason': str(e)}\n    \n    def _analyze_unsat_reason(self, unsat_core: List[str], all_assertions: List[str]) -> str:\n        \"\"\"Analyze unsat core to provide human-readable reasons\"\"\"\n        if not unsat_core:\n            return \"No specific unsat core available\"\n        \n        analysis_parts = []\n        core_text = ' '.join(unsat_core).lower()\n        \n        if len(unsat_core) == len(all_assertions):\n            analysis_parts.append(\"All constraints are involved in the conflict\")\n        else:\n            analysis_parts.append(f\"Conflict involves {len(unsat_core)} out of {len(all_assertions)} constraints\")\n        \n        if 'implies' in core_text and ('false' in core_text or 'not' in core_text):\n            analysis_parts.append(\"Contradiction in implication chain\")\n        if 'and' in core_text and 'not' in core_text:\n            analysis_parts.append(\"Direct contradiction between assertions\")\n        if 'forall' in core_text and 'exists' in core_text:\n            analysis_parts.append(\"Conflict between universal and existential quantifiers\")\n        if 'xor' in core_text:\n            analysis_parts.append(\"Exclusive OR conditions cannot be simultaneously satisfied\")\n        \n        if len(analysis_parts) == 1:\n            analysis_parts.append(\"General logical contradiction in constraints\")\n        \n        return \"; \".join(analysis_parts)\n\n\nclass AutomatedFOLVerificationPipeline:\n    \"\"\"Complete pipeline: NL-FOL → Axiom Discovery → Compounding → Z3 Verification\"\"\"\n    \n    def __init__(self, gemini_api_key: str):\n        self.axiom_discovery = GeminiAxiomDiscovery(gemini_api_key)\n        self.compound_builder = FOLCompoundBuilder()\n        self.fol_checker = EnterpriseFOLChecker(gemini_api_key)\n    \n    def verify_nl_fol_specifications(self, nl_fol_pairs: List[NLFOLPair]) -> SATResult:\n        \"\"\"\n        Complete verification pipeline:\n        1. Discover linking axioms from NL-FOL pairs\n        2. Build compound FOL formula\n        3. Verify with Z3\n        \"\"\"\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"AUTOMATED FOL VERIFICATION PIPELINE\")\n        print(\"=\"*70)\n        print(f\"\\nInput: {len(nl_fol_pairs)} NL-FOL specification pairs\")\n        \n        # Step 1: Discover linking axioms\n        linking_axioms = self.axiom_discovery.discover_linking_axioms(nl_fol_pairs)\n        \n        # Step 2: Build compound formula\n        compound_formula = self.compound_builder.build_compound_formula(nl_fol_pairs, linking_axioms)\n        \n        # Step 3: Verify with Z3\n        result = self.fol_checker.check_satisfiability(compound_formula)\n        \n        # Final summary\n        self._print_final_summary(nl_fol_pairs, linking_axioms, compound_formula, result)\n        \n        return result\n    \n    def _print_final_summary(self, nl_fol_pairs: List[NLFOLPair], \n                            linking_axioms: LinkingAxioms,\n                            compound_formula: str, \n                            result: SATResult):\n        \"\"\"Print comprehensive verification summary\"\"\"\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"VERIFICATION SUMMARY\")\n        print(\"=\"*70)\n        \n        print(f\"\\n📋 Original Specifications: {len(nl_fol_pairs)}\")\n        print(f\"🔗 Discovered Linking Axioms: {len(linking_axioms.axioms)}\")\n        print(f\"⚡ Total Formulas in Compound: {len(nl_fol_pairs) + len(linking_axioms.axioms)}\")\n        processing_time = result.processing_time if result is not None and result.processing_time is not None else 0.0\n        print(f\"⏱️  Processing Time: {processing_time:.3f}s\")\n        print(f\"✅ Status: {result.status}\")\n        \n        if result.status == \"SAT\":\n            print(\"\\n🎉 The specifications are SATISFIABLE\")\n            print(\"   A valid model exists that satisfies all constraints.\")\n            if result.model:\n                print(f\"\\n   Model: {result.model}\")\n                \n        elif result.status == \"UNSAT\":\n            print(\"\\n❌ The specifications are UNSATISFIABLE\")\n            print(\"   The constraints are contradictory and cannot all be true.\")\n            \n            if result.counterexample:\n                print(f\"\\n   Conflict Analysis: {result.counterexample.get('analysis', 'N/A')}\")\n                print(f\"   Conflicting Constraints: {result.counterexample.get('core_size', 0)}/{result.counterexample.get('total_assertions', 0)}\")\n                \n                if result.counterexample.get('unsat_core'):\n                    print(\"\\n   === Minimal Conflicting Set (Unsat Core) ===\")\n                    for i, constraint in enumerate(result.counterexample['unsat_core'], 1):\n                        print(f\"   {i}. {constraint}\")\n                        \n        elif result.status == \"ERROR\":\n            print(f\"\\n⚠️  Error occurred: {result.error_message}\")\n            \n        elif result.status == \"UNKNOWN\":\n            print(\"\\n❓ Z3 could not determine satisfiability\")\n        \n        print(\"\\n\" + \"=\"*70)\n\n\n# Test function with your example\ndef test_automated_verification():\n    \"\"\"Test the complete pipeline with your example\"\"\"\n    \n    api_key = \"api_key\"\n    \n    if not api_key:\n        raise ValueError(\"API key not provided\")\n    \n    # Define your NL-FOL pairs\n    nl_fol_pairs = [\n        NLFOLPair(\n            natural_language=\"System must guarantee an annualized return of exactly 10 percent for all customers regardless of their risk profile.\",\n            fol_formula=\"∀x∀y∀z ((System(x) ∧ Customer(y) ∧ RiskProfile(z)) → GuaranteesAnnualizedReturn(x, y, z, 10))\"\n        ),\n        NLFOLPair(\n            natural_language=\"For all customers, the system must guarantee a portfolio return of exactly 10 percent per annum.\",\n            fol_formula=\"∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10))\"\n        ),\n        NLFOLPair(\n            natural_language=\"For all high-risk customers, the system must ensure the portfolio return is strictly less than 10 percent per annum to comply with internal risk policies.\",\n            fol_formula=\"∀x (HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x))\"\n        )\n    ]\n    \n    # Create pipeline and run verification\n    pipeline = AutomatedFOLVerificationPipeline(api_key)\n    result = pipeline.verify_nl_fol_specifications(nl_fol_pairs)\n    \n    return result\n\n\nif __name__ == \"__main__\":\n    test_automated_verification()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T07:22:16.017724Z","iopub.execute_input":"2025-10-16T07:22:16.018027Z","iopub.status.idle":"2025-10-16T07:23:04.215345Z","shell.execute_reply.started":"2025-10-16T07:22:16.018007Z","shell.execute_reply":"2025-10-16T07:23:04.214558Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nAUTOMATED FOL VERIFICATION PIPELINE\n======================================================================\n\nInput: 3 NL-FOL specification pairs\n\n======================================================================\nSTEP 1: DISCOVERING LINKING AXIOMS\n======================================================================\n\nInput NL-FOL Pairs:\n\nRequirement 1:\nNatural Language: System must guarantee an annualized return of exactly 10 percent for all customers regardless of their risk profile.\nFOL Formula: ∀x∀y∀z ((System(x) ∧ Customer(y) ∧ RiskProfile(z)) → GuaranteesAnnualizedReturn(x, y, z, 10))\n\nRequirement 2:\nNatural Language: For all customers, the system must guarantee a portfolio return of exactly 10 percent per annum.\nFOL Formula: ∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10))\n\nRequirement 3:\nNatural Language: For all high-risk customers, the system must ensure the portfolio return is strictly less than 10 percent per annum to comply with internal risk policies.\nFOL Formula: ∀x (HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x))\n\n\n[DEBUG] Calling Gemini API for axiom discovery...\n[DEBUG] Prompt length: 7828 chars\n\n[DEBUG] ========== RAW GEMINI RESPONSE START ==========\n[\n  {\n    \"axiom\": \"∀c (HighRiskCustomer(c) → Customer(c))\",\n    \"explanation\": \"Type hierarchy: 'HighRiskCustomer' is a specialized type of 'Customer' based on natural language and common domain understanding.\"\n  },\n  {\n    \"axiom\": \"∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → ActualPortfolioReturn(c) = 10)\",\n    \"explanation\": \"Maps the abstract predicate 'GuaranteesAnnualizedReturn' to a concrete numeric value for the 'ActualPortfolioReturn' of a customer's portfolio, based on the 'exactly 10 percent' clause in Requirement 1.\"\n  },\n  {\n    \"axiom\": \"∀c (GuaranteedPortfolioReturn(c, 10) → ActualPortfolioReturn(c) = 10)\",\n    \"explanation\": \"Maps the abstract predicate 'GuaranteedPortfolioReturn' to a concrete numeric value for the 'ActualPortfolioReturn' of a customer's portfolio, based on the 'exactly 10 percent' clause in Requirement 2.\"\n  },\n  {\n    \"axiom\": \"∀c (PortfolioReturnLessThan10PercentPerAnnum(c) → ActualPortfolioReturn(c) < 10)\",\n    \"explanation\": \"Converts the abstract comparison predicate 'PortfolioReturnLessThan10PercentPerAnnum' into a concrete mathematical comparison using the 'ActualPortfolioReturn' and 'strictly less than 10 percent' clause in Requirement 3.\"\n  },\n  {\n    \"axiom\": \"∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → GuaranteedPortfolioReturn(c, 10))\",\n    \"explanation\": \"Unifies semantically equivalent predicates: 'GuaranteesAnnualizedReturn' (R1) and 'GuaranteedPortfolioReturn' (R2) both refer to the system guaranteeing a 10% annual return for a customer. The more specific predicate implies the general one.\"\n  },\n  {\n    \"axiom\": \"∃c (Customer(c))\",\n    \"explanation\": \"Existence constraint: Asserts that there must be at least one customer for the requirements to be non-vacuous and for potential inconsistencies to manifest.\"\n  },\n  {\n    \"axiom\": \"∃c (HighRiskCustomer(c))\",\n    \"explanation\": \"Existence constraint: Asserts that there must be at least one high-risk customer to test the specific policy in Requirement 3 and highlight the potential logical tension with Requirement 2.\"\n  }\n]\n[DEBUG] ========== RAW GEMINI RESPONSE END ==========\n[DEBUG] Response length: 2066 characters\n\n\n[DEBUG] Starting JSON parsing...\n[DEBUG] Extracted JSON array (length: 2066 chars)\n[DEBUG] ✓ JSON parsed successfully\n[DEBUG] Found 7 items\n  ✓ Axiom 1: ∀c (HighRiskCustomer(c) → Customer(c))...\n  ✓ Axiom 2: ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → ActualPortfolioRetur...\n  ✓ Axiom 3: ∀c (GuaranteedPortfolioReturn(c, 10) → ActualPortfolioReturn(c) = 10)...\n  ✓ Axiom 4: ∀c (PortfolioReturnLessThan10PercentPerAnnum(c) → ActualPortfolioRetur...\n  ✓ Axiom 5: ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → GuaranteedPortfolioR...\n  ✓ Axiom 6: ∃c (Customer(c))...\n  ✓ Axiom 7: ∃c (HighRiskCustomer(c))...\n\n[DEBUG] Parsing result:\n  - Number of axioms extracted: 7\n\n[SUCCESS] Discovered 7 linking axioms\n=== Discovered Linking Axioms ===\n1. ∀c (HighRiskCustomer(c) → Customer(c))\n   Explanation: Type hierarchy: 'HighRiskCustomer' is a specialized type of 'Customer' based on natural language and common domain understanding.\n2. ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → ActualPortfolioReturn(c) = 10)\n   Explanation: Maps the abstract predicate 'GuaranteesAnnualizedReturn' to a concrete numeric value for the 'ActualPortfolioReturn' of a customer's portfolio, based on the 'exactly 10 percent' clause in Requirement 1.\n3. ∀c (GuaranteedPortfolioReturn(c, 10) → ActualPortfolioReturn(c) = 10)\n   Explanation: Maps the abstract predicate 'GuaranteedPortfolioReturn' to a concrete numeric value for the 'ActualPortfolioReturn' of a customer's portfolio, based on the 'exactly 10 percent' clause in Requirement 2.\n4. ∀c (PortfolioReturnLessThan10PercentPerAnnum(c) → ActualPortfolioReturn(c) < 10)\n   Explanation: Converts the abstract comparison predicate 'PortfolioReturnLessThan10PercentPerAnnum' into a concrete mathematical comparison using the 'ActualPortfolioReturn' and 'strictly less than 10 percent' clause in Requirement 3.\n5. ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → GuaranteedPortfolioReturn(c, 10))\n   Explanation: Unifies semantically equivalent predicates: 'GuaranteesAnnualizedReturn' (R1) and 'GuaranteedPortfolioReturn' (R2) both refer to the system guaranteeing a 10% annual return for a customer. The more specific predicate implies the general one.\n6. ∃c (Customer(c))\n   Explanation: Existence constraint: Asserts that there must be at least one customer for the requirements to be non-vacuous and for potential inconsistencies to manifest.\n7. ∃c (HighRiskCustomer(c))\n   Explanation: Existence constraint: Asserts that there must be at least one high-risk customer to test the specific policy in Requirement 3 and highlight the potential logical tension with Requirement 2.\n\n\n======================================================================\nSTEP 2: BUILDING COMPOUND FOL FORMULA\n======================================================================\n\n[DEBUG] Adding original FOL formulas:\n  1. ∀x∀y∀z ((System(x) ∧ Customer(y) ∧ RiskProfile(z)) → GuaranteesAnnualizedReturn(x, y, z, 10))\n  2. ∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10))\n  3. ∀x (HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x))\n\n[DEBUG] Original formulas count: 3\n[DEBUG] Linking axioms count: 7\n\n[DEBUG] Adding linking axioms:\n  1. ∀c (HighRiskCustomer(c) → Customer(c))\n  2. ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → ActualPortfolioReturn(c) = 10)\n  3. ∀c (GuaranteedPortfolioReturn(c, 10) → ActualPortfolioReturn(c) = 10)\n  4. ∀c (PortfolioReturnLessThan10PercentPerAnnum(c) → ActualPortfolioReturn(c) < 10)\n  5. ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → GuaranteedPortfolioReturn(c, 10))\n  6. ∃c (Customer(c))\n  7. ∃c (HighRiskCustomer(c))\n\n[DEBUG] Total formulas before compounding: 10\n\n[SUCCESS] Compound formula created with 10 total formulas\n  - Original specifications: 3\n  - Linking axioms: 7\n\nCompound Formula (first 500 chars):\n----------------------------------------------------------------------\n∀x∀y∀z ((System(x) ∧ Customer(y) ∧ RiskProfile(z)) → GuaranteesAnnualizedReturn(x, y, z, 10)) ∧ ∀x (Customer(x) → GuaranteedPortfolioReturn(x, 10)) ∧ ∀x (HighRiskCustomer(x) → PortfolioReturnLessThan10PercentPerAnnum(x)) ∧ ∀c (HighRiskCustomer(c) → Customer(c)) ∧ ∀s∀c∀r (GuaranteesAnnualizedReturn(s, c, r, 10) → ActualPortfolioReturn(c) = 10) ∧ ∀c (GuaranteedPortfolioReturn(c, 10) → ActualPortfolioReturn(c) = 10) ∧ ∀c (PortfolioReturnLessThan10PercentPerAnnum(c) → ActualPortfolioReturn(c) < 10) ...\n----------------------------------------------------------------------\nFull formula length: 631 characters\n\n\n======================================================================\nSTEP 4: CHECKING SATISFIABILITY WITH Z3\n======================================================================\n\n======================================================================\nSTEP 3: TRANSLATING FOL TO Z3 CODE\n======================================================================\n\n[DEBUG] Calling Gemini API for Z3 translation...\n\n[DEBUG] Raw translation response received (2349 chars)\n\n[SUCCESS] Z3 code validation passed\n\n[DEBUG] Executing Z3 code...\n\n[RESULT] Formula is UNSATISFIABLE\n\n======================================================================\nVERIFICATION SUMMARY\n======================================================================\n\n📋 Original Specifications: 3\n🔗 Discovered Linking Axioms: 7\n⚡ Total Formulas in Compound: 10\n⏱️  Processing Time: 16.316s\n✅ Status: UNSAT\n\n❌ The specifications are UNSATISFIABLE\n   The constraints are contradictory and cannot all be true.\n\n   Conflict Analysis: Conflict involves 6 out of 10 constraints; General logical contradiction in constraints\n   Conflicting Constraints: 6/10\n\n   === Minimal Conflicting Set (Unsat Core) ===\n   1. rule2\n   2. rule6\n   3. rule4\n   4. rule3\n   5. rule7\n   6. rule10\n\n======================================================================\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}